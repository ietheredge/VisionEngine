{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from os import makedirs\n",
    "from numpy import zeros\n",
    "from numpy import ones\n",
    "from numpy import expand_dims\n",
    "from numpy import hstack\n",
    "from numpy.random import randn\n",
    "from numpy.random import randint\n",
    "# from tensorflow.keras.datasets.mnist import load_data\n",
    "from VisionEngine.data import butterflies\n",
    "from tensorflow.keras import backend\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Reshape\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Conv2DTranspose\n",
    "from tensorflow.keras.layers import UpSampling2D\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.constraints import Constraint\n",
    "from matplotlib import pyplot\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.executing_eagerly()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# x_train, x_test, y_train, y_test = train_test_split(images,lines,test_size=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.savez('guppy_ornaments', x_train=x_train, x_test=x_test, y_train=y_train, y_test=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# npz = np.load('guppy_ornaments.npz')\n",
    "# npz.files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # create a unique directory name for this script\n",
    "# def get_dir():\n",
    "#     import sys\n",
    "#     # get script path\n",
    "#     arg0 = sys.argv[0]\n",
    "#     # get filename\n",
    "#     filename = arg0.split('/')[-1]\n",
    "#     # remove extension\n",
    "#     filename = filename[:-3]\n",
    "#     return filename\n",
    "\n",
    "# # define the standalone discriminator model\n",
    "# def define_discriminator(n_cat, n_con, in_shape=(256,256,3)):\n",
    "#     # weight initialization\n",
    "#     init = RandomNormal(stddev=0.02)\n",
    "#     # image input\n",
    "#     in_image = Input(shape=in_shape)\n",
    "\n",
    "#     # 128x128\n",
    "#     d = Conv2D(64, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(in_image)\n",
    "#     d = LeakyReLU(alpha=0.1)(d)\n",
    "#     # 64x64\n",
    "#     d = Conv2D(64, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n",
    "#     d = LeakyReLU(alpha=0.1)(d)\n",
    "#     # 32x32\n",
    "#     d = Conv2D(64, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n",
    "#     d = LeakyReLU(alpha=0.1)(d)\n",
    "#     # 16x16\n",
    "#     d = Conv2D(128, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n",
    "#     d = LeakyReLU(alpha=0.1)(d)\n",
    "#     d = BatchNormalization()(d)\n",
    "#     # normal\n",
    "#     d = Conv2D(256, (4,4), padding='same', kernel_initializer=init)(d)\n",
    "#     d = LeakyReLU(alpha=0.1)(d)\n",
    "#     d = BatchNormalization()(d)\n",
    "\n",
    "#     # flatten feature maps\n",
    "#     d = Flatten()(d)\n",
    "#     # real/fake output\n",
    "#     out_classifier = Dense(1, activation='sigmoid')(d)\n",
    "#     # define d model\n",
    "#     d_model = Model(in_image, out_classifier)\n",
    "#     # compile d model\n",
    "#     d_model.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.0002, beta_1=0.5))\n",
    "#     # create q model layers\n",
    "#     q = Dense(128)(d)\n",
    "#     q = BatchNormalization()(q)\n",
    "#     q = LeakyReLU(alpha=0.1)(q)\n",
    "#     # q model output 1\n",
    "#     out_codes1 = Dense(n_cat, activation='softmax')(q)\n",
    "#     # q model output 2\n",
    "#     out_codes2 = Dense(n_con, activation='linear')(q)\n",
    "#     # define q model\n",
    "#     q_model = Model(in_image, [out_codes1, out_codes2])\n",
    "#     return d_model, q_model\n",
    "\n",
    "# # define the standalone generator model\n",
    "# def define_generator(gen_input_size):\n",
    "#     # weight initialization\n",
    "#     init = RandomNormal(stddev=0.02)\n",
    "#     # image generator input\n",
    "#     in_lat = Input(shape=(gen_input_size,))\n",
    "#     # foundation for 16x16 image\n",
    "#     n_nodes = 512 * 16 * 16\n",
    "#     gen = Dense(n_nodes, kernel_initializer=init)(in_lat)\n",
    "#     gen = Activation('relu')(gen)\n",
    "#     gen = BatchNormalization()(gen)\n",
    "#     gen = Reshape((16, 16, 512))(gen)\n",
    "\n",
    "#     # normal\n",
    "#     gen = Conv2D(128, (4,4), padding='same', kernel_initializer=init)(gen)\n",
    "#     gen = Activation('relu')(gen)\n",
    "#     gen = BatchNormalization()(gen)\n",
    "#     # 32x32\n",
    "#     gen = Conv2DTranspose(64, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(gen)\n",
    "#     gen = Activation('relu')(gen)\n",
    "#     gen = BatchNormalization()(gen)\n",
    "#     # 64x64\n",
    "#     gen = Conv2DTranspose(64, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(gen)\n",
    "#     gen = Activation('relu')(gen)\n",
    "#     gen = BatchNormalization()(gen)\n",
    "#     # 128x128\n",
    "#     gen = Conv2DTranspose(64, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(gen)\n",
    "#     gen = Activation('relu')(gen)\n",
    "#     gen = BatchNormalization()(gen)\n",
    "#     # 256x256\n",
    "#     gen = Conv2DTranspose(3, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(gen)\n",
    "\n",
    "#     # tanh output\n",
    "#     out_layer = Activation('tanh')(gen)\n",
    "#     # define model\n",
    "#     model = Model(in_lat, out_layer)\n",
    "#     return model\n",
    "\n",
    "# # define the combined discriminator, generator and q network model\n",
    "# def define_gan(g_model, d_model, q_model):\n",
    "#     # make weights in the discriminator (some shared with the q model) as not trainable\n",
    "#     d_model.trainable = False\n",
    "#     # connect g outputs to d inputs\n",
    "#     d_output = d_model(g_model.output)\n",
    "#     # connect g outputs to q inputs\n",
    "#     q_output = q_model(g_model.output)\n",
    "#     # define composite model\n",
    "#     model = Model(g_model.input, [d_output]+q_output)\n",
    "#     # compile model\n",
    "#     opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "#     model.compile(loss=['binary_crossentropy', 'categorical_crossentropy', 'mse'], optimizer=opt)\n",
    "#     d_model.compile(opt, loss='binary_crossentropy')\n",
    "#     return model\n",
    "\n",
    "# # load images\n",
    "# def load_real_samples():\n",
    "#     # load dataset\n",
    "#     (trainX, _), (_, _) = load_data()\n",
    "#     # expand to 3d, e.g. add channels\n",
    "# #     X = expand_dims(trainX, axis=-1)\n",
    "#     # convert from ints to floats\n",
    "#     X = trainX.astype('float32')\n",
    "#     # scale from [0,255] to [-1,1]\n",
    "#     X = (X - 127.5) / 127.5\n",
    "#     print(X.shape)\n",
    "#     return X\n",
    "\n",
    "# # select real samples\n",
    "# def generate_real_samples(dataset, n_samples):\n",
    "#     # choose random instances\n",
    "#     ix = randint(0, dataset.shape[0], n_samples)\n",
    "#     # select images and labels\n",
    "#     X = dataset[ix]\n",
    "#     # generate class labels\n",
    "#     y = ones((n_samples, 1))\n",
    "#     return X, y\n",
    "\n",
    "# # generate points in latent space as input for the generator\n",
    "# def generate_latent_points(latent_dim, n_cat, n_con, n_samples):\n",
    "#     # generate points in the latent space\n",
    "#     z_latent = randn(latent_dim * n_samples)\n",
    "#     # reshape into a batch of inputs for the network\n",
    "#     z_latent = z_latent.reshape(n_samples, latent_dim)\n",
    "#     # generate categorical codes\n",
    "#     cat_codes = randint(0, n_cat, n_samples)\n",
    "#     # one hot encode\n",
    "#     cat_codes = to_categorical(cat_codes, num_classes=n_cat)\n",
    "#     # generate continuous codes\n",
    "#     con_codes = randn(n_con * n_samples)\n",
    "#     # reshape continuous codes\n",
    "#     con_codes = con_codes.reshape(n_samples, n_con)\n",
    "#     # concatenate latent points and control codes\n",
    "#     z_input = hstack((z_latent, cat_codes, con_codes))\n",
    "#     return [z_input, cat_codes, con_codes]\n",
    "\n",
    "# # use the generator to generate n fake examples, with class labels\n",
    "# def generate_fake_samples(generator, latent_dim, n_cat, n_con, n_samples):\n",
    "#     # generate points in latent space and control codes\n",
    "#     z_input, _, _ = generate_latent_points(latent_dim, n_cat, n_con, n_samples)\n",
    "#     # predict outputs\n",
    "#     images = generator.predict(z_input)\n",
    "#     # create class labels\n",
    "#     y = zeros((n_samples, 1))\n",
    "#     return images, y\n",
    "\n",
    "# # generate samples and save as a plot and save the model\n",
    "# def summarize_performance(step, g_model, gan_model, latent_dim, n_cat, n_con, dataset, n_samples=100):\n",
    "#     # prepare fake examples\n",
    "#     X, _ = generate_fake_samples(g_model, latent_dim, n_cat, n_con, n_samples)\n",
    "#     # scale from [-1,1] to [0,1]\n",
    "#     X = (X + 1) / 2.0\n",
    "#     # plot images\n",
    "#     for i in range(100):\n",
    "#         # define subplot\n",
    "#         pyplot.subplot(10, 10, 1 + i)\n",
    "#         # turn off axis\n",
    "#         pyplot.axis('off')\n",
    "#         # plot raw pixel data\n",
    "#         pyplot.imshow(X[i, :, :, :])\n",
    "#     # save plot to file\n",
    "#     filename1 = '%s/generated_plot_%04d.png' % (get_dir(), (step+1))\n",
    "#     pyplot.savefig(filename1)\n",
    "#     pyplot.close()\n",
    "# #     Y, _ = generate_real_samples(dataset, n_samples)\n",
    "# #     for i in range(100):\n",
    "# #         # define subplot\n",
    "# #         pyplot.subplot(10, 10, 1 + i)\n",
    "# #         # turn off axis\n",
    "# #         pyplot.axis('off')\n",
    "# #         # plot raw pixel data\n",
    "# #         pyplot.imshow(Y[i, :, :, :])\n",
    "# #     # save plot to file\n",
    "# #     filename1 = '%s/real_plot_%04d.png' % (get_dir(), (step+1))\n",
    "# #     pyplot.savefig(filename1)\n",
    "# #     pyplot.close()\n",
    "#     # save the generator model\n",
    "#     filename2 = '%s/model_%04d.h5' % (get_dir(), (step+1))\n",
    "#     g_model.save(filename2)\n",
    "#     # save the gan model\n",
    "#     filename3 = '%s/gan_model_%04d.h5' % (get_dir(), (step+1))\n",
    "#     gan_model.save(filename3)\n",
    "#     print('>Saved: %s, %s, and %s' % (filename1, filename2, filename3))\n",
    "\n",
    "# # train the generator and discriminator\n",
    "# def train(g_model, d_model, gan_model, dataset, latent_dim, n_cat, n_con, n_epochs=5000, n_batch=64):\n",
    "#     # calculate the number of batches per training epoch\n",
    "#     bat_per_epo = int(dataset.shape[0] / n_batch)\n",
    "#     # calculate the number of training iterations\n",
    "#     n_steps = bat_per_epo * n_epochs\n",
    "#     # calculate the size of half a batch of samples\n",
    "#     half_batch = int(n_batch / 2)\n",
    "#     # manually enumerate epochs\n",
    "#     for i in range(n_steps):\n",
    "#         # get randomly selected 'real' samples\n",
    "#         X_real, y_real = generate_real_samples(dataset, half_batch)\n",
    "#         # update discriminator and q model weights\n",
    "#         d_loss1 = d_model.train_on_batch(X_real, y_real)\n",
    "#         # generate 'fake' examples\n",
    "#         X_fake, y_fake = generate_fake_samples(g_model, latent_dim, n_cat, n_con, half_batch)\n",
    "#         # update discriminator model weights\n",
    "#         d_loss2 = d_model.train_on_batch(X_fake, y_fake)\n",
    "#         # prepare points in latent space as input for the generator\n",
    "#         z_input, cat_codes, con_codes = generate_latent_points(latent_dim, n_cat, n_con, n_batch)\n",
    "#         # create inverted labels for the fake samples\n",
    "#         y_gan = ones((n_batch, 1))\n",
    "#         # update the g via the d and q error\n",
    "#         _,g_1,g_2,g3 = gan_model.train_on_batch(z_input, [y_gan, cat_codes, con_codes])\n",
    "#         # summarize loss on this batch\n",
    "#         # evaluate the model performance every 'epoch'\n",
    "#         if (i+1) % (bat_per_epo * 100) == 0:\n",
    "#             print('>%d, d[%.3f,%.3f], g[%.3f] q1[%.3f], q2[%.3f]' % (i+1, d_loss1, d_loss2, g_1, g_2, g3))\n",
    "#             summarize_performance(i, g_model, gan_model, latent_dim, n_cat, n_con, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # make folder for images\n",
    "# makedirs(get_dir(), exist_ok=True)\n",
    "# # number of categorical control codes\n",
    "# n_cat = 13\n",
    "# # number of continuous control codes\n",
    "# n_con = 32\n",
    "# # size of the latent space\n",
    "# latent_dim = 100\n",
    "# # create the discriminator\n",
    "# d_model, q_model = define_discriminator(n_cat, n_con)\n",
    "# # create the generator\n",
    "# gen_input_size = latent_dim + n_cat + n_con\n",
    "# g_model = define_generator(gen_input_size)\n",
    "# # create the gan\n",
    "# gan_model = define_gan(g_model, d_model, q_model)\n",
    "# # load image data\n",
    "# dataset = load_real_samples()\n",
    "# # train model\n",
    "# train(g_model, d_model, gan_model, dataset, latent_dim, n_cat, n_con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # define the standalone discriminator model\n",
    "# # def define_discriminator(n_cat, in_shape=(28,28,1)):\n",
    "# #     # weight initialization\n",
    "# #     init = RandomNormal(stddev=0.02)\n",
    "# #     # image input\n",
    "# #     in_image = Input(shape=in_shape)\n",
    "# #     # downsample to 14x14\n",
    "# #     d = Conv2D(64, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(in_image)\n",
    "# #     d = LeakyReLU(alpha=0.1)(d)\n",
    "# #     # downsample to 7x7\n",
    "# #     d = Conv2D(128, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n",
    "# #     d = LeakyReLU(alpha=0.1)(d)\n",
    "# #     d = BatchNormalization()(d)\n",
    "# #     # normal\n",
    "# #     d = Conv2D(256, (4,4), padding='same', kernel_initializer=init)(d)\n",
    "# #     d = LeakyReLU(alpha=0.1)(d)\n",
    "# #     d = BatchNormalization()(d)\n",
    "# #     # flatten feature maps\n",
    "# #     d = Flatten()(d)\n",
    "# #     # real/fake output\n",
    "# #     out_classifier = Dense(1, activation='sigmoid')(d)\n",
    "# #     # define d model\n",
    "# #     d_model = Model(in_image, out_classifier)\n",
    "# #     # compile d model\n",
    "# #     d_model.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.0002, beta_1=0.5))\n",
    "# #     # create q model layers\n",
    "# #     q = Dense(128)(d)\n",
    "# #     q = BatchNormalization()(q)\n",
    "# #     q = LeakyReLU(alpha=0.1)(q)\n",
    "# #     # q model output\n",
    "# #     out_codes = Dense(n_cat, activation='softmax')(q)\n",
    "# #     # define q model\n",
    "# #     q_model = Model(in_image, out_codes)\n",
    "# #     return d_model, q_model\n",
    "\n",
    "# def wasserstein_loss(y_true, y_pred):\n",
    "#     return K.mean(y_true * y_pred)\n",
    "\n",
    "# # define the standalone discriminator model\n",
    "# def define_discriminator(n_cat, in_shape=(256,256,3)):\n",
    "#     # weight initialization\n",
    "#     init = RandomNormal(stddev=0.02)\n",
    "#     # image input\n",
    "#     in_image = Input(shape=in_shape)\n",
    "\n",
    "#     # 128x128\n",
    "#     d = Conv2D(64, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(in_image)\n",
    "#     d = LeakyReLU(alpha=0.1)(d)\n",
    "#     # 64x64\n",
    "#     d = Conv2D(64, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n",
    "#     d = LeakyReLU(alpha=0.1)(d)\n",
    "#     # 32x32\n",
    "#     d = Conv2D(64, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n",
    "#     d = LeakyReLU(alpha=0.1)(d)\n",
    "#     # 16x16\n",
    "#     d = Conv2D(128, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n",
    "#     d = LeakyReLU(alpha=0.1)(d)\n",
    "#     d = BatchNormalization()(d)\n",
    "#     # normal\n",
    "#     d = Conv2D(256, (4,4), padding='same', kernel_initializer=init)(d)\n",
    "#     d = LeakyReLU(alpha=0.1)(d)\n",
    "#     d = BatchNormalization()(d)\n",
    "\n",
    "#     # flatten feature maps\n",
    "#     d = Flatten()(d)\n",
    "#     # real/fake output\n",
    "#     out_classifier = Dense(1, activation='sigmoid')(d)\n",
    "#     # define d model\n",
    "#     d_model = Model(in_image, out_classifier)\n",
    "#     # compile d model\n",
    "#     d_model.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.0002, beta_1=0.5))\n",
    "#     # create q model layers\n",
    "#     q = Dense(128)(d)\n",
    "#     q = BatchNormalization()(q)\n",
    "#     q = LeakyReLU(alpha=0.1)(q)\n",
    "#     # q model output 1\n",
    "#     out_codes = Dense(n_cat, activation='softmax')(q)\n",
    "#     # define q model\n",
    "#     q_model = Model(in_image, out_codes)\n",
    "#     return d_model, q_model\n",
    "\n",
    "# # # define the standalone generator model\n",
    "# #     # weight initialization\n",
    "# #     init = RandomNormal(stddev=0.02)\n",
    "# #     # image generator input\n",
    "# #     in_lat = Input(shape=(gen_input_size,))\n",
    "# #     # foundation for 7x7 image\n",
    "# #     n_nodes = 512 * 7 * 7\n",
    "# #     gen = Dense(n_nodes, kernel_initializer=init)(in_lat)\n",
    "# #     gen = Activation('relu')(gen)\n",
    "# #     gen = BatchNormalization()(gen)\n",
    "# #     gen = Reshape((7, 7, 512))(gen)\n",
    "# #     # normal\n",
    "# #     gen = Conv2D(128, (4,4), padding='same', kernel_initializer=init)(gen)\n",
    "# #     gen = Activation('relu')(gen)\n",
    "# #     gen = BatchNormalization()(gen)\n",
    "# #     # upsample to 14x14\n",
    "# #     gen = Conv2DTranspose(64, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(gen)\n",
    "# #     gen = Activation('relu')(gen)\n",
    "# #     gen = BatchNormalization()(gen)\n",
    "# #     # upsample to 28x28\n",
    "# #     gen = Conv2DTranspose(1, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(gen)\n",
    "# #     # tanh output\n",
    "# #     out_layer = Activation('tanh')(gen)\n",
    "# #     # define model\n",
    "# #     model = Model(in_lat, out_layer)\n",
    "# #     return model\n",
    "\n",
    "# # define the standalone generator model\n",
    "# def define_generator(gen_input_size):\n",
    "#     # weight initialization\n",
    "#     init = RandomNormal(stddev=0.02)\n",
    "#     # image generator input\n",
    "#     in_lat = Input(shape=(gen_input_size,))\n",
    "#     # foundation for 16x16 image\n",
    "#     n_nodes = 512 * 16 * 16\n",
    "#     gen = Dense(n_nodes, kernel_initializer=init)(in_lat)\n",
    "#     gen = Activation('relu')(gen)\n",
    "#     gen = BatchNormalization()(gen)\n",
    "#     gen = Reshape((16, 16, 512))(gen)\n",
    "\n",
    "#     # normal\n",
    "#     gen = Conv2D(128, (4,4), padding='same', kernel_initializer=init)(gen)\n",
    "#     gen = Activation('relu')(gen)\n",
    "#     gen = BatchNormalization()(gen)\n",
    "#     # 32x32\n",
    "#     gen = Conv2DTranspose(64, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(gen)\n",
    "#     gen = Activation('relu')(gen)\n",
    "#     gen = BatchNormalization()(gen)\n",
    "#     # 64x64\n",
    "#     gen = Conv2DTranspose(64, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(gen)\n",
    "#     gen = Activation('relu')(gen)\n",
    "#     gen = BatchNormalization()(gen)\n",
    "#     # 128x128\n",
    "#     gen = Conv2DTranspose(64, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(gen)\n",
    "#     gen = Activation('relu')(gen)\n",
    "#     gen = BatchNormalization()(gen)\n",
    "#     # 256x256\n",
    "#     gen = Conv2DTranspose(3, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(gen)\n",
    "\n",
    "#     # tanh output\n",
    "#     out_layer = Activation('tanh')(gen)\n",
    "#     # define model\n",
    "#     model = Model(in_lat, out_layer)\n",
    "#     return model\n",
    "\n",
    "# # define the combined discriminator, generator and q network model\n",
    "# def define_gan(g_model, d_model, q_model):\n",
    "#     # make weights in the discriminator (some shared with the q model) as not trainable\n",
    "#     d_model.trainable = False\n",
    "#     # connect g outputs to d inputs\n",
    "#     d_output = d_model(g_model.output)\n",
    "#     # connect g outputs to q inputs\n",
    "#     q_output = q_model(g_model.output)\n",
    "#     # define composite model\n",
    "#     model = Model(g_model.input, [d_output, q_output])\n",
    "#     # compile model\n",
    "#     opt = Adam()\n",
    "#     model.compile(loss=['binary_crossentropy', 'categorical_crossentropy'], optimizer=opt)\n",
    "#     d_model.compile(opt, loss='binary_crossentropy')\n",
    "#     return model\n",
    "\n",
    "# # load images\n",
    "# def load_real_samples():\n",
    "#     # load dataset\n",
    "#     (trainX, _), (_, _) = load_data()\n",
    "#     # expand to 3d, e.g. add channels\n",
    "# #     X = expand_dims(trainX, axis=-1)\n",
    "#     # convert from ints to floats\n",
    "#     X = trainX.astype('float32')\n",
    "#     # scale from [0,255] to [-1,1]\n",
    "# #     X = (X - 127.5) / 127.5\n",
    "#     print(X.shape)\n",
    "#     return X\n",
    "\n",
    "# # select real samples\n",
    "# def generate_real_samples(dataset, n_samples):\n",
    "#     # choose random instances\n",
    "#     ix = randint(0, dataset.shape[0], n_samples)\n",
    "#     # select images and labels\n",
    "#     X = dataset[ix]\n",
    "#     # generate class labels\n",
    "#     y = ones((n_samples, 1))\n",
    "#     return X, y\n",
    "\n",
    "# # generate points in latent space as input for the generator\n",
    "# def generate_latent_points(latent_dim, n_cat, n_samples):\n",
    "#     # generate points in the latent space\n",
    "#     z_latent = randn(latent_dim * n_samples)\n",
    "#     # reshape into a batch of inputs for the network\n",
    "#     z_latent = z_latent.reshape(n_samples, latent_dim)\n",
    "#     # generate categorical codes\n",
    "#     cat_codes = randint(0, n_cat, n_samples)\n",
    "#     # one hot encode\n",
    "#     cat_codes = to_categorical(cat_codes, num_classes=n_cat)\n",
    "#     # concatenate latent points and control codes\n",
    "#     z_input = hstack((z_latent, cat_codes))\n",
    "#     return [z_input, cat_codes]\n",
    "\n",
    "# # use the generator to generate n fake examples, with class labels\n",
    "# def generate_fake_samples(generator, latent_dim, n_cat, n_samples):\n",
    "#     # generate points in latent space and control codes\n",
    "#     z_input, _ = generate_latent_points(latent_dim, n_cat, n_samples)\n",
    "#     # predict outputs\n",
    "#     images = generator.predict(z_input)\n",
    "#     # create class labels\n",
    "#     y = zeros((n_samples, 1))\n",
    "#     return images, y\n",
    "\n",
    "# # generate samples and save as a plot and save the model\n",
    "# def summarize_performance(step, g_model, gan_model, latent_dim, n_cat, n_samples=100):\n",
    "#     # prepare fake examples\n",
    "#     X, _ = generate_fake_samples(g_model, latent_dim, n_cat, n_samples)\n",
    "#     X = (X + 1) / 2.0\n",
    "#     # plot images\n",
    "#     for i in range(100):\n",
    "#         # define subplot\n",
    "#         pyplot.subplot(10, 10, 1 + i)\n",
    "#         # turn off axis\n",
    "#         pyplot.axis('off')\n",
    "#         # plot raw pixel data\n",
    "#         pyplot.imshow(X[i, :, :, :])\n",
    "#     # save plot to file\n",
    "#     filename1 = '%s/generated_plot_%04d.png' % (get_dir(), (step+1))\n",
    "#     pyplot.savefig(filename1)\n",
    "#     pyplot.close()\n",
    "#     # save the generator model\n",
    "#     filename2 = 'model_%04d.h5' % (step+1)\n",
    "#     g_model.save(filename2)\n",
    "#     # save the gan model\n",
    "#     filename3 = 'gan_model_%04d.h5' % (step+1)\n",
    "#     gan_model.save(filename3)\n",
    "#     print('>Saved: %s, %s, and %s' % (filename1, filename2, filename3))\n",
    "\n",
    "# # train the generator and discriminator\n",
    "# def train(g_model, d_model, gan_model, dataset, latent_dim, n_cat, n_epochs=5000, n_batch=32):\n",
    "#     # calculate the number of batches per training epoch\n",
    "#     bat_per_epo = int(dataset.shape[0] / n_batch)\n",
    "#     # calculate the number of training iterations\n",
    "#     n_steps = bat_per_epo * n_epochs\n",
    "#     # calculate the size of half a batch of samples\n",
    "#     half_batch = int(n_batch / 2)\n",
    "#     # manually enumerate epochs\n",
    "#     for i in range(n_steps):\n",
    "#         # get randomly selected 'real' samples\n",
    "#         X_real, y_real = generate_real_samples(dataset, half_batch)\n",
    "#         # update discriminator and q model weights\n",
    "#         d_loss1 = d_model.train_on_batch(X_real, y_real)\n",
    "#         # generate 'fake' examples\n",
    "#         X_fake, y_fake = generate_fake_samples(g_model, latent_dim, n_cat, half_batch)\n",
    "#         # update discriminator model weights\n",
    "#         d_loss2 = d_model.train_on_batch(X_fake, y_fake)\n",
    "#         # prepare points in latent space as input for the generator\n",
    "#         z_input, cat_codes = generate_latent_points(latent_dim, n_cat, n_batch)\n",
    "#         # create inverted labels for the fake samples\n",
    "#         y_gan = ones((n_batch, 1))\n",
    "#         # update the g via the d and q error\n",
    "#         _,g_1,g_2 = gan_model.train_on_batch(z_input, [y_gan, cat_codes])\n",
    "#         # summarize loss on this batch\n",
    "        \n",
    "#         # evaluate the model performance every 'epoch'\n",
    "#         if (i+1) % (bat_per_epo * 100) == 0:\n",
    "#             print('>%d, d[%.3f,%.3f], g[%.3f] q[%.3f]' % (i+1, d_loss1, d_loss2, g_1, g_2))\n",
    "#             summarize_performance(i, g_model, gan_model, latent_dim, n_cat)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# makedirs(get_dir(), exist_ok=True)\n",
    "# # number of values for the categorical control code\n",
    "# n_cat = 13\n",
    "# # size of the latent space\n",
    "# latent_dim = 100\n",
    "# # create the discriminator\n",
    "# d_model, q_model = define_discriminator(n_cat)\n",
    "# # create the generator\n",
    "# gen_input_size = latent_dim + n_cat\n",
    "# g_model = define_generator(gen_input_size)\n",
    "# # create the gan\n",
    "# gan_model = define_gan(g_model, d_model, q_model)\n",
    "# # load image data\n",
    "# dataset = load_real_samples()\n",
    "# # train model\n",
    "# train(g_model, d_model, gan_model, dataset, latent_dim, n_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# from keras.layers import Activation, Dense, Input\n",
    "# from keras.layers import Conv2D, Flatten\n",
    "# from keras.layers import Reshape, Conv2DTranspose\n",
    "# from keras.layers import LeakyReLU\n",
    "# from keras.layers import BatchNormalization\n",
    "# from keras.models import Model\n",
    "# from keras.layers.merge import concatenate\n",
    "\n",
    "# import numpy as np\n",
    "# import math\n",
    "# import matplotlib.pyplot as plt\n",
    "# import os\n",
    "\n",
    "# def generator(inputs,\n",
    "#               image_size,\n",
    "#               activation='sigmoid',\n",
    "#               labels=None,\n",
    "#               codes=None):\n",
    "#     \"\"\"Build a Generator Model\n",
    "#     Stack of BN-ReLU-Conv2DTranpose to generate fake images.\n",
    "#     Output activation is sigmoid instead of tanh in [1].\n",
    "#     Sigmoid converges easily.\n",
    "#     # Arguments\n",
    "#         inputs (Layer): Input layer of the generator (the z-vector)\n",
    "#         image_size (int): Target size of one side (assuming square image)\n",
    "#         activation (string): Name of output activation layer\n",
    "#         labels (tensor): Input labels\n",
    "#         codes (list): 2-dim disentangled codes for InfoGAN\n",
    "#     # Returns\n",
    "#         Model: Generator Model\n",
    "#     \"\"\"\n",
    "#     image_resize = image_size // 4\n",
    "#     # network parameters\n",
    "#     kernel_size = 5\n",
    "#     layer_filters = [128, 64, 32, 3]\n",
    "\n",
    "#     if labels is not None:\n",
    "#         if codes is None:\n",
    "#             # ACGAN labels\n",
    "#             # concatenate z noise vector and one-hot labels\n",
    "#             inputs = [inputs, labels]\n",
    "#         else:\n",
    "#             # infoGAN codes\n",
    "#             # concatenate z noise vector, one-hot labels and codes 1 & 2\n",
    "#             inputs = [inputs, labels] + codes\n",
    "#         x = concatenate(inputs, axis=1)\n",
    "#     elif codes is not None:\n",
    "#         # generator 0 of StackedGAN\n",
    "#         inputs = [inputs, codes]\n",
    "#         x = concatenate(inputs, axis=1)\n",
    "#     else:\n",
    "#         # default input is just 100-dim noise (z-code)\n",
    "#         x = inputs\n",
    "\n",
    "#     x = Dense(image_resize * image_resize * layer_filters[0])(x)\n",
    "#     x = Reshape((image_resize, image_resize, layer_filters[0]))(x)\n",
    "\n",
    "#     for filters in layer_filters:\n",
    "#         # first two convolution layers use strides = 2\n",
    "#         # the last two use strides = 1\n",
    "#         if filters > layer_filters[-2]:\n",
    "#             strides = 2\n",
    "#         else:\n",
    "#             strides = 1\n",
    "#         x = BatchNormalization()(x)\n",
    "#         x = Activation('relu')(x)\n",
    "#         x = Conv2DTranspose(filters=filters,\n",
    "#                             kernel_size=kernel_size,\n",
    "#                             strides=strides,\n",
    "#                             padding='same')(x)\n",
    "\n",
    "#     if activation is not None:\n",
    "#         x = Activation(activation)(x)\n",
    "\n",
    "#     # generator output is the synthesized image x\n",
    "#     return Model(inputs, x, name='generator')\n",
    "\n",
    "\n",
    "# def discriminator(inputs,\n",
    "#                   activation='sigmoid',\n",
    "#                   num_labels=13,\n",
    "#                   num_codes=2):\n",
    "#     \"\"\"Build a Discriminator Model\n",
    "#     Stack of LeakyReLU-Conv2D to discriminate real from fake\n",
    "#     The network does not converge with BN so it is not used here\n",
    "#     unlike in [1]\n",
    "#     # Arguments\n",
    "#         inputs (Layer): Input layer of the discriminator (the image)\n",
    "#         activation (string): Name of output activation layer\n",
    "#         num_labels (int): Dimension of one-hot labels for ACGAN & InfoGAN\n",
    "#         num_codes (int): num_codes-dim Q network as output \n",
    "#                     if StackedGAN or 2 Q networks if InfoGAN\n",
    "                    \n",
    "#     # Returns\n",
    "#         Model: Discriminator Model\n",
    "#     \"\"\"\n",
    "#     kernel_size = 5\n",
    "#     layer_filters = [32, 64, 128, 256]\n",
    "\n",
    "#     x = inputs\n",
    "#     for filters in layer_filters:\n",
    "#         # first 3 convolution layers use strides = 2\n",
    "#         # last one uses strides = 1\n",
    "#         if filters == layer_filters[-1]:\n",
    "#             strides = 1\n",
    "#         else:\n",
    "#             strides = 2\n",
    "        \n",
    "#         x = Conv2D(filters=filters,\n",
    "#                    kernel_size=kernel_size,\n",
    "#                    strides=strides,\n",
    "#                    padding='same')(x)\n",
    "#         x = LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "#     x = Flatten()(x)\n",
    "#     # default output is probability that the image is real\n",
    "#     outputs = Dense(1)(x)\n",
    "#     if activation is not None:\n",
    "#         print(activation)\n",
    "#         outputs = Activation(activation)(outputs)\n",
    "\n",
    "#     if num_labels:\n",
    "#         # ACGAN and InfoGAN have 2nd output\n",
    "#         # 2nd output is 10-dim one-hot vector of label\n",
    "#         layer = Dense(layer_filters[-2])(x)\n",
    "#         labels = Dense(num_labels)(layer)\n",
    "#         labels = Activation('softmax', name='label')(labels)\n",
    "#         if num_codes is None:\n",
    "#             outputs = [outputs, labels]\n",
    "#         else:\n",
    "#             # InfoGAN have 3rd and 4th outputs\n",
    "#             # 3rd output is 1-dim continous Q of 1st c given x\n",
    "#             code1 = Dense(1)(layer)\n",
    "#             code1 = Activation('sigmoid', name='code1')(code1)\n",
    "\n",
    "#             # 4th output is 1-dim continuous Q of 2nd c given x\n",
    "#             code2 = Dense(1)(layer)\n",
    "#             code2 = Activation('sigmoid', name='code2')(code2)\n",
    "\n",
    "#             outputs = [outputs, labels, code1, code2]\n",
    "#     elif num_codes is not None:\n",
    "#         # StackedGAN Q0 output\n",
    "#         # z0_recon is reconstruction of z0 normal distribution\n",
    "#         z0_recon =  Dense(num_codes)(x)\n",
    "#         z0_recon = Activation('tanh', name='z0')(z0_recon)\n",
    "#         outputs = [outputs, z0_recon]\n",
    "\n",
    "#     return Model(inputs, outputs, name='discriminator')\n",
    "\n",
    "\n",
    "# def train(models, x_train, params):\n",
    "#     \"\"\"Train the Discriminator and Adversarial Networks\n",
    "#     Alternately train Discriminator and Adversarial networks by batch.\n",
    "#     Discriminator is trained first with properly real and fake images.\n",
    "#     Adversarial is trained next with fake images pretending to be real\n",
    "#     Generate sample images per save_interval.\n",
    "#     # Arguments\n",
    "#         models (list): Generator, Discriminator, Adversarial models\n",
    "#         x_train (tensor): Train images\n",
    "#         params (list) : Networks parameters\n",
    "#     \"\"\"\n",
    "#     # the GAN models\n",
    "#     generator, discriminator, adversarial = models\n",
    "#     # network parameters\n",
    "#     batch_size, latent_size, train_steps, model_name = params\n",
    "#     # the generator image is saved every 500 steps\n",
    "#     save_interval = 500\n",
    "#     # noise vector to see how the generator output evolves during training\n",
    "#     noise_input = np.random.uniform(-1.0, 1.0, size=[16, latent_size])\n",
    "#     # number of elements in train dataset\n",
    "#     train_size = x_train.shape[0]\n",
    "#     for i in range(train_steps):\n",
    "#         # train the discriminator for 1 batch\n",
    "#         # 1 batch of real (label=1.0) and fake images (label=0.0)\n",
    "#         # randomly pick real images from dataset\n",
    "#         rand_indexes = np.random.randint(0, train_size, size=batch_size)\n",
    "#         real_images = x_train[rand_indexes]\n",
    "#         # generate fake images from noise using generator \n",
    "#         # generate noise using uniform distribution\n",
    "#         noise = np.random.uniform(-1.0, 1.0, size=[batch_size, latent_size])\n",
    "#         # generate fake images\n",
    "#         fake_images = generator.predict(noise)\n",
    "#         # real + fake images = 1 batch of train data\n",
    "#         x = np.concatenate((real_images, fake_images))\n",
    "#         # label real and fake images\n",
    "#         # real images label is 1.0\n",
    "#         y = np.ones([2 * batch_size, 1])\n",
    "#         # fake images label is 0.0\n",
    "#         y[batch_size:, :] = 0.0\n",
    "#         # train discriminator network, log the loss and accuracy\n",
    "#         loss, acc = discriminator.train_on_batch(x, y)\n",
    "#         log = \"%d: [discriminator loss: %f, acc: %f]\" % (i, loss, acc)\n",
    "\n",
    "#         # train the adversarial network for 1 batch\n",
    "#         # 1 batch of fake images with label=1.0\n",
    "#         # since the discriminator weights are frozen in adversarial network\n",
    "#         # only the generator is trained\n",
    "#         # generate noise using uniform distribution\n",
    "#         noise = np.random.uniform(-1.0, 1.0, size=[batch_size, latent_size])\n",
    "#         # label fake images as real or 1.0\n",
    "#         y = np.ones([batch_size, 1])\n",
    "#         # train the adversarial network \n",
    "#         # note that unlike in discriminator training, \n",
    "#         # we do not save the fake images in a variable\n",
    "#         # the fake images go to the discriminator input of the adversarial\n",
    "#         # for classification\n",
    "#         # log the loss and accuracy\n",
    "#         loss, acc = adversarial.train_on_batch(noise, y)\n",
    "#         log = \"%s [adversarial loss: %f, acc: %f]\" % (log, loss, acc)\n",
    "#         print(log)\n",
    "#         if (i + 1) % save_interval == 0:\n",
    "#             if (i + 1) == train_steps:\n",
    "#                 show = True\n",
    "#             else:\n",
    "#                 show = False\n",
    "\n",
    "#             # plot generator images on a periodic basis\n",
    "#             plot_images(generator,\n",
    "#                         noise_input=noise_input,\n",
    "#                         show=show,\n",
    "#                         step=(i + 1),\n",
    "#                         model_name=model_name)\n",
    "   \n",
    "#     # save the model after training the generator\n",
    "#     # the trained generator can be reloaded for future MNIST digit generation\n",
    "#     generator.save(model_name + \".h5\")\n",
    "\n",
    "\n",
    "# def plot_images(generator,\n",
    "#                 noise_input,\n",
    "#                 noise_label=None,\n",
    "#                 noise_codes=None,\n",
    "#                 show=False,\n",
    "#                 step=0,\n",
    "#                 model_name=\"gan\"):\n",
    "#     \"\"\"Generate fake images and plot them\n",
    "#     For visualization purposes, generate fake images\n",
    "#     then plot them in a square grid\n",
    "#     # Arguments\n",
    "#         generator (Model): The Generator Model for fake images generation\n",
    "#         noise_input (ndarray): Array of z-vectors\n",
    "#         show (bool): Whether to show plot or not\n",
    "#         step (int): Appended to filename of the save images\n",
    "#         model_name (string): Model name\n",
    "#     \"\"\"\n",
    "#     os.makedirs(model_name, exist_ok=True)\n",
    "#     filename = os.path.join(model_name, \"%05d.png\" % step)\n",
    "#     rows = int(math.sqrt(noise_input.shape[0]))\n",
    "#     if noise_label is not None:\n",
    "#         noise_input = [noise_input, noise_label]\n",
    "#         if noise_codes is not None:\n",
    "#             noise_input += noise_codes\n",
    "\n",
    "#     images = generator.predict(noise_input)\n",
    "#     plt.figure(figsize=(10, 10))\n",
    "#     num_images = images.shape[0]\n",
    "#     image_size = images.shape[1]\n",
    "#     for i in range(num_images):\n",
    "#         plt.subplot(rows, rows, i + 1)\n",
    "#         image = np.reshape(images[i], [image_size, image_size, 3])\n",
    "#         plt.imshow(image)\n",
    "#         plt.axis('off')\n",
    "#     plt.savefig(filename)\n",
    "#     if show:\n",
    "#         plt.show()\n",
    "#     else:\n",
    "#         plt.close('all')\n",
    "\n",
    "\n",
    "# def test_generator(generator):\n",
    "#     noise_input = np.random.uniform(-1.0, 1.0, size=[16, 100])\n",
    "#     plot_images(generator,\n",
    "#                 noise_input=noise_input,\n",
    "#                 show=True,\n",
    "#                 model_name=\"test_outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from __future__ import absolute_import\n",
    "# from __future__ import division\n",
    "# from __future__ import print_function\n",
    "\n",
    "# from keras.layers import Input\n",
    "# from keras.optimizers import RMSprop\n",
    "# from keras.models import Model\n",
    "# from keras.datasets import mnist\n",
    "# from keras.utils import to_categorical\n",
    "# from keras.models import load_model\n",
    "# from keras import backend as K\n",
    "\n",
    "# import numpy as np\n",
    "# import argparse\n",
    "\n",
    "# import sys\n",
    "# sys.path.append(\"..\")\n",
    "\n",
    "\n",
    "# # from ..lib import gan\n",
    "\n",
    "# def train(models, data, params):\n",
    "#     \"\"\"Train the Discriminator and Adversarial networks\n",
    "#     Alternately train discriminator and adversarial networks by batch.\n",
    "#     Discriminator is trained first with real and fake images,\n",
    "#     corresponding one-hot labels and continuous codes.\n",
    "#     Adversarial is trained next with fake images pretending to be real,\n",
    "#     corresponding one-hot labels and continous codes.\n",
    "#     Generate sample images per save_interval.\n",
    "#     # Arguments\n",
    "#         models (Models): Generator, Discriminator, Adversarial models\n",
    "#         data (tuple): x_train, y_train data\n",
    "#         params (tuple): Network parameters\n",
    "#     \"\"\"\n",
    "#     # the GAN models\n",
    "#     generator, discriminator, adversarial = models\n",
    "#     # images and their one-hot labels\n",
    "#     x_train, y_train = data\n",
    "#     # network parameters\n",
    "#     batch_size, latent_size, train_steps, num_labels, model_name = params\n",
    "#     # the generator image is saved every 500 steps\n",
    "#     save_interval = 500\n",
    "#     # noise vector to see how the generator output evolves during training\n",
    "#     noise_input = np.random.uniform(-1.0, 1.0, size=[16, latent_size])\n",
    "#     # random class labels and codes\n",
    "#     noise_label = np.eye(num_labels)[np.arange(0, 16) % num_labels]\n",
    "#     noise_code1 = np.random.normal(scale=0.5, size=[16, 1])\n",
    "#     noise_code2 = np.random.normal(scale=0.5, size=[16, 1])\n",
    "#     # number of elements in train dataset\n",
    "#     train_size = x_train.shape[0]\n",
    "#     print(model_name,\n",
    "#           \"Labels for generated images: \",\n",
    "#           np.argmax(noise_label, axis=1))\n",
    "\n",
    "#     for i in range(train_steps):\n",
    "#         # train the discriminator for 1 batch\n",
    "#         # 1 batch of real (label=1.0) and fake images (label=0.0)\n",
    "#         # randomly pick real images and corresponding labels from dataset \n",
    "#         rand_indexes = np.random.randint(0, train_size, size=batch_size)\n",
    "#         real_images = x_train[rand_indexes]\n",
    "#         real_labels = y_train[rand_indexes]\n",
    "#         # random codes for real images\n",
    "#         real_code1 = np.random.normal(scale=0.5, size=[batch_size, 1])\n",
    "#         real_code2 = np.random.normal(scale=0.5, size=[batch_size, 1])\n",
    "#         # generate fake images, labels and codes\n",
    "#         noise = np.random.uniform(-1.0, 1.0, size=[batch_size, latent_size])\n",
    "#         fake_labels = np.eye(num_labels)[np.random.choice(num_labels,\n",
    "#                                                           batch_size)]\n",
    "#         fake_code1 = np.random.normal(scale=0.5, size=[batch_size, 1])\n",
    "#         fake_code2 = np.random.normal(scale=0.5, size=[batch_size, 1])\n",
    "#         inputs = [noise, fake_labels, fake_code1, fake_code2]\n",
    "#         fake_images = generator.predict(inputs)\n",
    "\n",
    "#         # real + fake images = 1 batch of train data\n",
    "#         x = np.concatenate((real_images, fake_images))\n",
    "#         labels = np.concatenate((real_labels, fake_labels))\n",
    "#         codes1 = np.concatenate((real_code1, fake_code1))\n",
    "#         codes2 = np.concatenate((real_code2, fake_code2))\n",
    "\n",
    "#         # label real and fake images\n",
    "#         # real images label is 1.0\n",
    "#         y = np.ones([2 * batch_size, 1])\n",
    "#         # fake images label is 0.0\n",
    "#         y[batch_size:, :] = 0\n",
    "\n",
    "#         # train discriminator network, log the loss and label accuracy\n",
    "#         outputs = [y, labels, codes1, codes2]\n",
    "#         # metrics = ['loss', 'activation_1_loss', 'label_loss',\n",
    "#         # 'code1_loss', 'code2_loss', 'activation_1_acc',\n",
    "#         # 'label_acc', 'code1_acc', 'code2_acc']\n",
    "#         # from discriminator.metrics_names\n",
    "#         metrics = discriminator.train_on_batch(x, outputs)\n",
    "#         fmt = \"%d: [discriminator loss: %f, label_acc: %f]\"\n",
    "#         log = fmt % (i, metrics[0], metrics[6])\n",
    "\n",
    "#         # train the adversarial network for 1 batch\n",
    "#         # 1 batch of fake images with label=1.0 and\n",
    "#         # corresponding one-hot label or class + random codes\n",
    "#         # since the discriminator weights are frozen in adversarial network\n",
    "#         # only the generator is trained\n",
    "#         # generate fake images, labels and codes\n",
    "#         noise = np.random.uniform(-1.0, 1.0, size=[batch_size, latent_size])\n",
    "#         fake_labels = np.eye(num_labels)[np.random.choice(num_labels,\n",
    "#                                                           batch_size)]\n",
    "#         fake_code1 = np.random.normal(scale=0.5, size=[batch_size, 1])\n",
    "#         fake_code2 = np.random.normal(scale=0.5, size=[batch_size, 1])\n",
    "#         # label fake images as real\n",
    "#         y = np.ones([batch_size, 1])\n",
    "\n",
    "#         # train the adversarial network \n",
    "#         # note that unlike in discriminator training, \n",
    "#         # we do not save the fake images in a variable\n",
    "#         # the fake images go to the discriminator input of the adversarial\n",
    "#         # for classification\n",
    "#         # log the loss and label accuracy\n",
    "#         inputs = [noise, fake_labels, fake_code1, fake_code2]\n",
    "#         outputs = [y, fake_labels, fake_code1, fake_code2]\n",
    "#         metrics  = adversarial.train_on_batch(inputs, outputs)\n",
    "#         fmt = \"%s [adversarial loss: %f, label_acc: %f]\"\n",
    "#         log = fmt % (log, metrics[0], metrics[6])\n",
    "\n",
    "#         print(log)\n",
    "#         if (i + 1) % save_interval == 0:\n",
    "#             if (i + 1) == train_steps:\n",
    "#                 show = True\n",
    "#             else:\n",
    "#                 show = False\n",
    "    \n",
    "#             # plot generator images on a periodic basis\n",
    "#             plot_images(generator,\n",
    "#                             noise_input=noise_input,\n",
    "#                             noise_label=noise_label,\n",
    "#                             noise_codes=[noise_code1, noise_code2],\n",
    "#                             show=show,\n",
    "#                             step=(i + 1),\n",
    "#                             model_name=model_name)\n",
    "   \n",
    "#     # save the model after training the generator\n",
    "#     # the trained generator can be reloaded for\n",
    "#     # future MNIST digit generation\n",
    "#     generator.save(model_name + \".h5\")\n",
    "\n",
    "\n",
    "# def mi_loss(c, q_of_c_given_x):\n",
    "#     \"\"\" Mutual information, Equation 5 in [2] , assuming H(c) is constant\"\"\"\n",
    "#     # mi_loss = -c * log(Q(c|x))\n",
    "#     return K.mean(-K.sum(K.log(q_of_c_given_x + K.epsilon()) * c, axis=1))\n",
    "\n",
    "\n",
    "# def test_generator(generator, params, latent_size=100):\n",
    "#     label, code1, code2, p1, p2 = params\n",
    "#     noise_input = np.random.uniform(-1.0, 1.0, size=[16, latent_size])\n",
    "#     step = 0\n",
    "#     if label is None:\n",
    "#         num_labels = 13\n",
    "#         noise_label = np.eye(num_labels)[np.random.choice(num_labels, 16)]\n",
    "#     else:\n",
    "#         noise_label = np.zeros((16, 13))\n",
    "#         noise_label[:,label] = 1\n",
    "#         step = label\n",
    "\n",
    "#     if code1 is None:\n",
    "#         noise_code1 = np.random.normal(scale=0.5, size=[16, 1])\n",
    "#     else:\n",
    "#         if p1:\n",
    "#             a = np.linspace(-2, 2, 16)\n",
    "#             a = np.reshape(a, [16, 1])\n",
    "#             noise_code1 = np.ones((16, 1)) * a\n",
    "#         else:\n",
    "#             noise_code1 = np.ones((16, 1)) * code1\n",
    "#         print(noise_code1)\n",
    "\n",
    "#     if code2 is None:\n",
    "#         noise_code2 = np.random.normal(scale=0.5, size=[16, 1])\n",
    "#     else:\n",
    "#         if p2:\n",
    "#             a = np.linspace(-2, 2, 16)\n",
    "#             a = np.reshape(a, [16, 1])\n",
    "#             noise_code2 = np.ones((16, 1)) * a\n",
    "#         else:\n",
    "#             noise_code2 = np.ones((16, 1)) * code2\n",
    "#         print(noise_code2)\n",
    "\n",
    "#     plot_images(generator,\n",
    "#                     noise_input=noise_input,\n",
    "#                     noise_label=noise_label,\n",
    "#                     noise_codes=[noise_code1, noise_code2],\n",
    "#                     show=True,\n",
    "#                     step=step,\n",
    "#                     model_name=\"test_outputs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"infogan_guppies\"\n",
    "\n",
    "# # load dataset\n",
    "# (x_train, y_train), (_, _) = load_data()\n",
    "\n",
    "# # reshape data for CNN as (28, 28, 1) and normalize\n",
    "# image_size = x_train.shape[1]\n",
    "# x_train = np.reshape(x_train, [-1, image_size, image_size, 3])\n",
    "# x_train = x_train.astype('float32')\n",
    "\n",
    "\n",
    "# ####\n",
    "# #plotting sanity check\n",
    "\n",
    "# os.makedirs(model_name, exist_ok=True)\n",
    "# filename = os.path.join(model_name, \"example_real_inputs.png\")\n",
    "# rows = int(math.sqrt(16))\n",
    "# images = x_train[0:16]\n",
    "# plt.figure(figsize=(10, 10))\n",
    "# num_images = 16\n",
    "# image_size = 256\n",
    "# for i in range(num_images):\n",
    "#     plt.subplot(rows, rows, i + 1)\n",
    "#     image = np.reshape(images[i], [image_size, image_size, 3])\n",
    "#     plt.imshow(image)\n",
    "#     plt.axis('off')\n",
    "# plt.savefig(filename)\n",
    "# plt.close('all')\n",
    "# ####\n",
    "\n",
    "# # train labels\n",
    "# num_labels = len(np.unique(y_train))\n",
    "# print(num_labels)\n",
    "# _, y_train = np.unique(y_train,return_inverse=True)\n",
    "# y_train = to_categorical(y_train)\n",
    "\n",
    "# # network parameters\n",
    "# batch_size = 32\n",
    "# train_steps = 40000\n",
    "# lr = 2e-4\n",
    "# decay = 6e-8\n",
    "# input_shape = (image_size, image_size, 3)\n",
    "# label_shape = (num_labels, )\n",
    "# code_shape = (1, )\n",
    "# latent_size = 100\n",
    "\n",
    "# # build discriminator model\n",
    "# inputs = Input(shape=input_shape, name='discriminator_input')\n",
    "# # call discriminator builder with 4 outputs: source, label, and 2 codes\n",
    "# discriminator = discriminator(inputs,\n",
    "#                                   num_labels=num_labels,\n",
    "#                                   num_codes=2)\n",
    "# # [1] uses Adam, but discriminator converges easily with RMSprop\n",
    "# optimizer = RMSprop(lr=lr, decay=decay)\n",
    "# # loss functions: 1) probability image is real (binary crossentropy)\n",
    "# # 2) categorical cross entropy image label,\n",
    "# # 3) and 4) mutual information loss\n",
    "# loss = ['binary_crossentropy', 'categorical_crossentropy', mi_loss, mi_loss]\n",
    "# # lamda or mi_loss weight is 0.5\n",
    "# loss_weights = [1.0, 1.0, 0.5, 0.5]\n",
    "# discriminator.compile(loss=loss,\n",
    "#                       loss_weights=loss_weights,\n",
    "#                       optimizer=optimizer,\n",
    "#                       metrics=['accuracy'])\n",
    "# discriminator.summary()\n",
    "\n",
    "# # build generator model\n",
    "# input_shape = (latent_size, )\n",
    "# inputs = Input(shape=input_shape, name='z_input')\n",
    "# labels = Input(shape=label_shape, name='labels')\n",
    "# code1 = Input(shape=code_shape, name=\"code1\")\n",
    "# code2 = Input(shape=code_shape, name=\"code2\")\n",
    "# # call generator with inputs, \n",
    "# # labels and codes as total inputs to generator\n",
    "# generator = generator(inputs,\n",
    "#                           image_size,\n",
    "#                           labels=labels,\n",
    "#                           codes=[code1, code2])\n",
    "# generator.summary()\n",
    "\n",
    "# # build adversarial model = generator + discriminator\n",
    "# optimizer = RMSprop(lr=lr*0.5, decay=decay*0.5)\n",
    "# discriminator.trainable = False\n",
    "# # total inputs = noise code, labels, and codes\n",
    "# inputs = [inputs, labels, code1, code2]\n",
    "# adversarial = Model(inputs,\n",
    "#                     discriminator(generator(inputs)),\n",
    "#                     name=model_name)\n",
    "# # same loss as discriminator\n",
    "# adversarial.compile(loss=loss,\n",
    "#                     loss_weights=loss_weights,\n",
    "#                     optimizer=optimizer,\n",
    "#                     metrics=['accuracy'])\n",
    "# adversarial.summary()\n",
    "\n",
    "# # train discriminator and adversarial networks\n",
    "# models = (generator, discriminator, adversarial)\n",
    "# data = (x_train, y_train)\n",
    "# params = (batch_size, latent_size, train_steps, num_labels, model_name)\n",
    "# train(models, data, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator = load_model(\"infogan_guppies.h5\")\n",
    "# label = 1\n",
    "# code1 = -2\n",
    "# code2 = None\n",
    "# p1 = False\n",
    "# p2 = False\n",
    "# params = (label, code1, code2, p1, p2)\n",
    "# test_generator(generator, params, latent_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "def generator(inputs,\n",
    "              image_size,\n",
    "              activation='sigmoid',\n",
    "              labels=None,\n",
    "              codes=None):\n",
    "    \"\"\"Build a Generator Model\n",
    "    Stack of BN-ReLU-Conv2DTranpose to generate fake images.\n",
    "    Output activation is sigmoid instead of tanh in [1].\n",
    "    Sigmoid converges easily.\n",
    "    # Arguments\n",
    "        inputs (Layer): Input layer of the generator (the z-vector)\n",
    "        image_size (int): Target size of one side (assuming square image)\n",
    "        activation (string): Name of output activation layer\n",
    "        labels (tensor): Input labels\n",
    "        codes (list): 2-dim disentangled codes for InfoGAN\n",
    "    # Returns\n",
    "        Model: Generator Model\n",
    "    \"\"\"\n",
    "    image_resize = image_size // 4\n",
    "    # network parameters\n",
    "    kernel_size = 4\n",
    "    layer_filters = [128, 64, 32, 3]\n",
    "\n",
    "    if labels is not None:\n",
    "        if codes is None:\n",
    "            # ACGAN labels\n",
    "            # concatenate z noise vector and one-hot labels\n",
    "            inputs = [inputs, labels]\n",
    "        else:\n",
    "            # infoGAN codes\n",
    "            # concatenate z noise vector, one-hot labels and codes 1 & 2\n",
    "            inputs = [inputs, labels] + codes\n",
    "        x = concatenate(inputs, axis=1)\n",
    "    elif codes is not None:\n",
    "        # generator 0 of StackedGAN\n",
    "        inputs = [inputs, codes]\n",
    "        x = concatenate(inputs, axis=1)\n",
    "    else:\n",
    "        # default input is just 100-dim noise (z-code)\n",
    "        x = inputs\n",
    "\n",
    "    x = Dense(image_resize * image_resize * layer_filters[0])(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Reshape((image_resize, image_resize, layer_filters[0]))(x)\n",
    "\n",
    "    for filters in layer_filters:\n",
    "        # first two convolution layers use strides = 2\n",
    "        # the last two use strides = 1\n",
    "        if filters > layer_filters[-2]:\n",
    "            strides = 2\n",
    "        else:\n",
    "            strides = 1\n",
    "        x = UpSampling2D(size=2)(x)\n",
    "        x = Conv2D(filters=filters,\n",
    "                            kernel_size=kernel_size,\n",
    "                            strides=strides,\n",
    "                            padding='same')(x)\n",
    "        x = BatchNormalization(momentum=0.8)(x)\n",
    "        x = Activation(activation)(x)\n",
    "\n",
    "#     if activation is not None:\n",
    "#         x = Activation(activation)(x)\n",
    "\n",
    "    # generator output is the synthesized image x\n",
    "    return Model(inputs, x, name='generator')\n",
    "\n",
    "\n",
    "# def discriminator(inputs,\n",
    "#                   activation='sigmoid',\n",
    "#                   num_labels=13,\n",
    "#                   num_codes=None):\n",
    "#     \"\"\"Build a Discriminator Model\n",
    "#     Stack of LeakyReLU-Conv2D to discriminate real from fake\n",
    "#     The network does not converge with BN so it is not used here\n",
    "#     unlike in [1]\n",
    "#     # Arguments\n",
    "#         inputs (Layer): Input layer of the discriminator (the image)\n",
    "#         activation (string): Name of output activation layer\n",
    "#         num_labels (int): Dimension of one-hot labels for ACGAN & InfoGAN\n",
    "#         num_codes (int): num_codes-dim Q network as output \n",
    "#                     if StackedGAN or 2 Q networks if InfoGAN\n",
    "                    \n",
    "#     # Returns\n",
    "#         Model: Discriminator Model\n",
    "#     \"\"\"\n",
    "#     kernel_size = 4\n",
    "#     layer_filters = [32, 64, 128, 256]\n",
    "\n",
    "#     x = inputs\n",
    "#     for filters in layer_filters:\n",
    "#         # first 3 convolution layers use strides = 2\n",
    "#         # last one uses strides = 1\n",
    "#         if filters == layer_filters[-1]:\n",
    "#             strides = 1\n",
    "#         else:\n",
    "#             strides = 2\n",
    "        \n",
    "#         x = Conv2D(filters=filters,\n",
    "#                    kernel_size=kernel_size,\n",
    "#                    strides=strides,\n",
    "#                    padding='same')(x)\n",
    "#         x = LeakyReLU(alpha=0.2)(x)\n",
    "#         x = Dropout(0.25)(x)\n",
    "\n",
    "#     x = Flatten()(x)\n",
    "#     # default output is probability that the image is real\n",
    "#     outputs = Dense(1)(x)\n",
    "#     if activation is not None:\n",
    "#         print(activation)\n",
    "#         outputs = Activation(activation)(outputs)\n",
    "\n",
    "#     if num_labels:\n",
    "#         layer = Dense(layer_filters[-2])(x)\n",
    "#         labels = Dense(num_labels)(layer)\n",
    "#         labels = Activation('softmax', name='label')(labels)\n",
    "#         if num_codes is None:\n",
    "#             outputs = [outputs, labels]\n",
    "#         else:\n",
    "#             for code_i in range(num_codes+1):\n",
    "#                 code = Dense(1)(layer)\n",
    "#                 code = Activation('sigmoid', name='code{}'.format(code_i))(code)\n",
    "#                 outputs = [outputs, code]\n",
    "    \n",
    "#     elif num_codes is not None:\n",
    "\n",
    "#         z0_recon =  Dense(num_codes)(x)\n",
    "#         z0_recon = Activation('tanh', name='z0')(z0_recon)\n",
    "#         outputs = [outputs, z0_recon]\n",
    "#     outputs = [item for sublist in outputs for item in sublist]\n",
    "#     return Model(inputs, outputs, name='discriminator')\n",
    "\n",
    "def discriminator(inputs,\n",
    "                  activation='sigmoid',\n",
    "                  num_labels=8,\n",
    "                  num_codes=None):\n",
    "    \"\"\"Build a Discriminator Model\n",
    "    Stack of LeakyReLU-Conv2D to discriminate real from fake\n",
    "    The network does not converge with BN so it is not used here\n",
    "    unlike in [1]\n",
    "    # Arguments\n",
    "        inputs (Layer): Input layer of the discriminator (the image)\n",
    "        activation (string): Name of output activation layer\n",
    "        num_labels (int): Dimension of one-hot labels for ACGAN & InfoGAN\n",
    "        num_codes (int): num_codes-dim Q network as output \n",
    "                    if StackedGAN or 2 Q networks if InfoGAN\n",
    "                    \n",
    "    # Returns\n",
    "        Model: Discriminator Model\n",
    "    \"\"\"\n",
    "    kernel_size = 4\n",
    "    layer_filters = [32, 64, 128, 256]\n",
    "\n",
    "    x = inputs\n",
    "    for filters in layer_filters:\n",
    "        # first 3 convolution layers use strides = 2\n",
    "        # last one uses strides = 1\n",
    "        if filters == layer_filters[-1]:\n",
    "            strides = 1\n",
    "        else:\n",
    "            strides = 2\n",
    "        \n",
    "        x = Conv2D(filters=filters,\n",
    "                   kernel_size=kernel_size,\n",
    "                   strides=strides,\n",
    "                   padding='same')(x)\n",
    "        x = LeakyReLU(alpha=0.2)(x)\n",
    "        x = Dropout(0.25)(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    # default output is probability that the image is real\n",
    "    output = Dense(1)(x)\n",
    "    if activation is not None:\n",
    "        print(activation)\n",
    "        output = Activation(activation)(output)\n",
    "    \n",
    "    outputs = []\n",
    "    outputs.append(output)\n",
    "    if num_labels:\n",
    "        # ACGAN and InfoGAN have 2nd output\n",
    "        # 2nd output is 10-dim one-hot vector of label\n",
    "        layer = Dense(layer_filters[-2])(x)\n",
    "        labels = Dense(num_labels)(layer)\n",
    "        labels = Activation('softmax', name='label')(labels)\n",
    "        outputs.append(labels)\n",
    "        for code_i in range(num_codes):\n",
    "        # InfoGAN have 3rd and 4th outputs\n",
    "        # 3rd output is 1-dim continous Q of 1st c given x\n",
    "            code = Dense(1)(layer)\n",
    "            code = Activation('sigmoid', name='code{}'.format(code_i))(code)\n",
    "            outputs.append(code)\n",
    "    \n",
    "    elif num_codes is not None:\n",
    "        # StackedGAN Q0 output\n",
    "        # z0_recon is reconstruction of z0 normal distribution\n",
    "        z0_recon =  Dense(num_codes)(x)\n",
    "        z0_recon = Activation('tanh', name='z0')(z0_recon)\n",
    "        outputs.append(z0_recon)\n",
    "        \n",
    "    return Model(inputs, outputs, name='discriminator')\n",
    "\n",
    "\n",
    "\n",
    "def train(models, x_train, params):\n",
    "    \"\"\"Train the Discriminator and Adversarial Networks\n",
    "    Alternately train Discriminator and Adversarial networks by batch.\n",
    "    Discriminator is trained first with properly real and fake images.\n",
    "    Adversarial is trained next with fake images pretending to be real\n",
    "    Generate sample images per save_interval.\n",
    "    # Arguments\n",
    "        models (list): Generator, Discriminator, Adversarial models\n",
    "        x_train (tensor): Train images\n",
    "        params (list) : Networks parameters\n",
    "    \"\"\"\n",
    "    # the GAN models\n",
    "    generator, discriminator, adversarial = models\n",
    "    # network parameters\n",
    "    batch_size, latent_size, train_steps, model_name = params\n",
    "    # the generator image is saved every 500 steps\n",
    "    save_interval = 500\n",
    "    # noise vector to see how the generator output evolves during training\n",
    "    noise_input = np.random.uniform(-1.0, 1.0, size=[16, latent_size])\n",
    "    # number of elements in train dataset\n",
    "    train_size = x_train.shape[0]\n",
    "    for i in range(train_steps):\n",
    "        # train the discriminator for 1 batch\n",
    "        # 1 batch of real (label=1.0) and fake images (label=0.0)\n",
    "        # randomly pick real images from dataset\n",
    "        rand_indexes = np.random.randint(0, train_size, size=batch_size)\n",
    "        real_images = x_train[rand_indexes]\n",
    "        # generate fake images from noise using generator \n",
    "        # generate noise using uniform distribution\n",
    "        noise = np.random.uniform(-1.0, 1.0, size=[batch_size, latent_size])\n",
    "        # generate fake images\n",
    "        fake_images = generator.predict(noise)\n",
    "        # real + fake images = 1 batch of train data\n",
    "        x = np.concatenate((real_images, fake_images))\n",
    "        # label real and fake images\n",
    "        # real images label is 1.0\n",
    "        y = np.ones([2 * batch_size, 1])\n",
    "        # fake images label is 0.0\n",
    "        y[batch_size:, :] = 0.0\n",
    "        # train discriminator network, log the loss and accuracy\n",
    "        loss, acc = discriminator.train_on_batch(x, y)\n",
    "        log = \"%d: [discriminator loss: %f, acc: %f]\" % (i, loss, acc)\n",
    "\n",
    "        # train the adversarial network for 1 batch\n",
    "        # 1 batch of fake images with label=1.0\n",
    "        # since the discriminator weights are frozen in adversarial network\n",
    "        # only the generator is trained\n",
    "        # generate noise using uniform distribution\n",
    "        noise = np.random.uniform(-1.0, 1.0, size=[batch_size, latent_size])\n",
    "        # label fake images as real or 1.0\n",
    "        y = np.ones([batch_size, 1])\n",
    "        # train the adversarial network \n",
    "        # note that unlike in discriminator training, \n",
    "        # we do not save the fake images in a variable\n",
    "        # the fake images go to the discriminator input of the adversarial\n",
    "        # for classification\n",
    "        # log the loss and accuracy\n",
    "        loss, acc = adversarial.train_on_batch(noise, y)\n",
    "        log = \"%s [adversarial loss: %f, acc: %f]\" % (log, loss, acc)\n",
    "        print(log)\n",
    "        if (i + 1) % save_interval == 0:\n",
    "            if (i + 1) == train_steps:\n",
    "                show = True\n",
    "            else:\n",
    "                show = False\n",
    "\n",
    "            # plot generator images on a periodic basis\n",
    "            plot_images(generator,\n",
    "                        noise_input=noise_input,\n",
    "                        show=show,\n",
    "                        step=(i + 1),\n",
    "                        model_name=model_name)\n",
    "   \n",
    "    # save the model after training the generator\n",
    "    # the trained generator can be reloaded for future MNIST digit generation\n",
    "    generator.save(model_name + \".h5\")\n",
    "\n",
    "\n",
    "def plot_images(generator,\n",
    "                noise_input,\n",
    "                noise_label=None,\n",
    "                noise_codes=None,\n",
    "                show=False,\n",
    "                step=0,\n",
    "                model_name=\"gan\"):\n",
    "    \"\"\"Generate fake images and plot them\n",
    "    For visualization purposes, generate fake images\n",
    "    then plot them in a square grid\n",
    "    # Arguments\n",
    "        generator (Model): The Generator Model for fake images generation\n",
    "        noise_input (ndarray): Array of z-vectors\n",
    "        show (bool): Whether to show plot or not\n",
    "        step (int): Appended to filename of the save images\n",
    "        model_name (string): Model name\n",
    "    \"\"\"\n",
    "    os.makedirs(model_name, exist_ok=True)\n",
    "    filename = os.path.join(model_name, \"%05d.png\" % step)\n",
    "    rows = int(math.sqrt(noise_input.shape[0]))\n",
    "    if noise_label is not None:\n",
    "        noise_input = [noise_input, noise_label]\n",
    "        if noise_codes is not None:\n",
    "            noise_input += noise_codes\n",
    "\n",
    "    images = generator.predict(noise_input)\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    num_images = images.shape[0]\n",
    "    image_size = images.shape[1]\n",
    "    for i in range(num_images):\n",
    "        plt.subplot(rows, rows, i + 1)\n",
    "        image = np.reshape(images[i], [image_size, image_size, 3])\n",
    "        plt.imshow(image)\n",
    "        plt.axis('off')\n",
    "    plt.savefig(filename)\n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close('all')\n",
    "\n",
    "def test_generator(generator):\n",
    "    noise_input = np.random.uniform(-0.5, 0.5, size=[16, 100])\n",
    "    plot_images(generator,\n",
    "                noise_input=noise_input,\n",
    "                show=True,\n",
    "                model_name=\"test_outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import argparse\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "\n",
    "# from ..lib import gan\n",
    "\n",
    "# def train(models, data, params, num_codes=2):\n",
    "#     \"\"\"Train the Discriminator and Adversarial networks\n",
    "#     Alternately train discriminator and adversarial networks by batch.\n",
    "#     Discriminator is trained first with real and fake images,\n",
    "#     corresponding one-hot labels and continuous codes.\n",
    "#     Adversarial is trained next with fake images pretending to be real,\n",
    "#     corresponding one-hot labels and continous codes.\n",
    "#     Generate sample images per save_interval.\n",
    "#     # Arguments\n",
    "#         models (Models): Generator, Discriminator, Adversarial models\n",
    "#         data (tuple): x_train, y_train data\n",
    "#         params (tuple): Network parameters\n",
    "#     \"\"\"\n",
    "#     # the GAN models\n",
    "#     generator, discriminator, adversarial = models\n",
    "#     # images and their one-hot labels\n",
    "#     x_train, y_train = data\n",
    "#     # network parameters\n",
    "#     batch_size, latent_size, train_steps, num_labels, model_name = params\n",
    "#     print(num_labels)\n",
    "#     # the generator image is saved every 500 steps\n",
    "#     save_interval = 100\n",
    "#     # noise vector to see how the generator output evolves during training\n",
    "#     noise_input = np.random.uniform(-1.0, 1.0, size=[16, latent_size])\n",
    "#     # random class labels and codes\n",
    "#     noise_label = np.eye(num_labels)[np.arange(0, 16) % num_labels]\n",
    "    \n",
    "#     noise_codes = [np.random.normal(scale=0.5, size=[16, 1]) for i in range(num_codes)]\n",
    "# #     noise_code1 = np.random.normal(scale=0.5, size=[16, 1])\n",
    "# #     noise_code2 = np.random.normal(scale=0.5, size=[16, 1])\n",
    "#     # number of elements in train dataset\n",
    "#     train_size = x_train.shape[0]\n",
    "#     print(model_name,\n",
    "#           \"Labels for generated images: \",\n",
    "#           np.argmax(noise_label, axis=1))\n",
    "\n",
    "#     for i in range(train_steps):\n",
    "#         # train the discriminator for 1 batch\n",
    "#         # 1 batch of real (label=1.0) and fake images (label=0.0)\n",
    "#         # randomly pick real images and corresponding labels from dataset \n",
    "#         rand_indexes = np.random.randint(0, train_size, size=batch_size)\n",
    "#         real_images = x_train[rand_indexes]\n",
    "#         real_labels = y_train[rand_indexes]\n",
    "#         # random codes for real images\n",
    "#         real_codes = [np.random.normal(scale=0.5, size=[batch_size, 1]) for i in range(num_codes)]\n",
    "# #         real_code1 = np.random.normal(scale=0.5, size=[batch_size, 1])\n",
    "# #         real_code2 = np.random.normal(scale=0.5, size=[batch_size, 1])\n",
    "#         # generate fake images, labels and codes\n",
    "#         noise = np.random.uniform(-1.0, 1.0, size=[batch_size, latent_size])\n",
    "#         fake_labels = np.eye(num_labels)[np.random.choice(num_labels,\n",
    "#                                                           batch_size)]\n",
    "#         fake_codes = [np.random.normal(scale=0.5, size=[batch_size, 1]) for i in range(num_codes)]\n",
    "# #         fake_code1 = np.random.normal(scale=0.5, size=[batch_size, 1])\n",
    "# #         fake_code2 = np.random.normal(scale=0.5, size=[batch_size, 1])\n",
    "#         inputs = concatenate([noise, fake_labels, fake_codes])\n",
    "#         fake_images = generator.predict(inputs)\n",
    "\n",
    "#         # real + fake images = 1 batch of train data\n",
    "#         x = np.concatenate((real_images, fake_images))\n",
    "#         labels = np.concatenate((real_labels, fake_labels))\n",
    "#         codes = np.concatenate([(real_code, fake_code) for real_code, fake_code in zip(real_codes, fake_codes)])\n",
    "# #         codes1 = np.concatenate((real_code1, fake_code1))\n",
    "# #         codes2 = np.concatenate((real_code2, fake_code2))\n",
    "\n",
    "#         # label real and fake images\n",
    "#         # real images label is 1.0\n",
    "#         y = np.ones([2 * batch_size, 1])\n",
    "#         # fake images label is 0.0\n",
    "#         y[batch_size:, :] = 0\n",
    "\n",
    "#         # train discriminator network, log the loss and label accuracy\n",
    "#         outputs = concatenate([y, labels, codes])\n",
    "#         # metrics = ['loss', 'activation_1_loss', 'label_loss',\n",
    "#         # 'code1_loss', 'code2_loss', 'activation_1_acc',\n",
    "#         # 'label_acc', 'code1_acc', 'code2_acc']\n",
    "#         # from discriminator.metrics_names\n",
    "#         metrics = discriminator.train_on_batch(x, outputs)\n",
    "#         fmt = \"%d: [discriminator loss: %f, label_acc: %f]\"\n",
    "#         log = fmt % (i, metrics[0], metrics[6])\n",
    "\n",
    "#         # train the adversarial network for 1 batch\n",
    "#         # 1 batch of fake images with label=1.0 and\n",
    "#         # corresponding one-hot label or class + random codes\n",
    "#         # since the discriminator weights are frozen in adversarial network\n",
    "#         # only the generator is trained\n",
    "#         # generate fake images, labels and codes\n",
    "#         noise = np.random.uniform(-1.0, 1.0, size=[batch_size, latent_size])\n",
    "#         fake_labels = np.eye(num_labels)[np.random.choice(num_labels,\n",
    "#                                                           batch_size)]\n",
    "# #         fake_code1 = np.random.normal(scale=0.5, size=[batch_size, 1])\n",
    "# #         fake_code2 = np.random.normal(scale=0.5, size=[batch_size, 1])\n",
    "#         # label fake images as real\n",
    "#         y = np.ones([batch_size, 1])\n",
    "\n",
    "#         # train the adversarial network \n",
    "#         # note that unlike in discriminator training, \n",
    "#         # we do not save the fake images in a variable\n",
    "#         # the fake images go to the discriminator input of the adversarial\n",
    "#         # for classification\n",
    "#         # log the loss and label accuracy\n",
    "#         inputs = [noise, fake_labels] + [fake_code for fake_code in fake_codes]\n",
    "#         outputs = [y, fake_labels] + [fake_code for fake_code in fake_codes]\n",
    "#         metrics  = adversarial.train_on_batch(inputs, outputs)\n",
    "#         fmt = \"%s [adversarial loss: %f, label_acc: %f]\"\n",
    "#         log = fmt % (log, metrics[0], metrics[6])\n",
    "\n",
    "#         print(log)\n",
    "#         if (i + 1) % save_interval == 0:\n",
    "#             if (i + 1) == train_steps:\n",
    "#                 show = True\n",
    "#             else:\n",
    "#                 show = False\n",
    "    \n",
    "#             # plot generator images on a periodic basis\n",
    "#             plot_images(generator,\n",
    "#                             noise_input=noise_input,\n",
    "#                             noise_label=noise_label,\n",
    "#                             noise_codes=noise_codes,\n",
    "#                             show=show,\n",
    "#                             step=(i + 1),\n",
    "#                             model_name=model_name)\n",
    "   \n",
    "#     # save the model after training the generator\n",
    "#     # the trained generator can be reloaded for\n",
    "#     # future MNIST digit generation\n",
    "#     generator.save(model_name + \".h5\")\n",
    "def train(models, x_train, params):\n",
    "    \"\"\"Train the Discriminator and Adversarial Networks\n",
    "    Alternately train Discriminator and Adversarial networks by batch.\n",
    "    Discriminator is trained first with properly real and fake images.\n",
    "    Adversarial is trained next with fake images pretending to be real\n",
    "    Generate sample images per save_interval.\n",
    "    # Arguments\n",
    "        models (list): Generator, Discriminator, Adversarial models\n",
    "        x_train (tensor): Train images\n",
    "        params (list) : Networks parameters\n",
    "    \"\"\"\n",
    "    # the GAN models\n",
    "    generator, discriminator, adversarial = models\n",
    "    # network parameters\n",
    "    batch_size, latent_size, train_steps, model_name = params\n",
    "    # the generator image is saved every 500 steps\n",
    "    save_interval = 500\n",
    "    # noise vector to see how the generator output evolves during training\n",
    "    noise_input = np.random.uniform(-1.0, 1.0, size=[16, latent_size])\n",
    "    # number of elements in train dataset\n",
    "    train_size = x_train.shape[0]\n",
    "    for i in range(train_steps):\n",
    "        # train the discriminator for 1 batch\n",
    "        # 1 batch of real (label=1.0) and fake images (label=0.0)\n",
    "        # randomly pick real images from dataset\n",
    "        rand_indexes = np.random.randint(0, train_size, size=batch_size)\n",
    "        real_images = x_train[rand_indexes]\n",
    "        # generate fake images from noise using generator \n",
    "        # generate noise using uniform distribution\n",
    "        noise = np.random.uniform(-1.0, 1.0, size=[batch_size, latent_size])\n",
    "        # generate fake images\n",
    "        fake_images = generator.predict(noise)\n",
    "        # real + fake images = 1 batch of train data\n",
    "        x = np.concatenate((real_images, fake_images))\n",
    "        # label real and fake images\n",
    "        # real images label is 1.0\n",
    "        y = np.ones([2 * batch_size, 1])\n",
    "        # fake images label is 0.0\n",
    "        y[batch_size:, :] = 0.0\n",
    "        # train discriminator network, log the loss and accuracy\n",
    "        loss, acc = discriminator.train_on_batch(x, y)\n",
    "        log = \"%d: [discriminator loss: %f, acc: %f]\" % (i, loss, acc)\n",
    "\n",
    "        # train the adversarial network for 1 batch\n",
    "        # 1 batch of fake images with label=1.0\n",
    "        # since the discriminator weights are frozen in adversarial network\n",
    "        # only the generator is trained\n",
    "        # generate noise using uniform distribution\n",
    "        noise = np.random.uniform(-1.0, 1.0, size=[batch_size, latent_size])\n",
    "        # label fake images as real or 1.0\n",
    "        y = np.ones([batch_size, 1])\n",
    "        # train the adversarial network \n",
    "        # note that unlike in discriminator training, \n",
    "        # we do not save the fake images in a variable\n",
    "        # the fake images go to the discriminator input of the adversarial\n",
    "        # for classification\n",
    "        # log the loss and accuracy\n",
    "        loss, acc = adversarial.train_on_batch(noise, y)\n",
    "        log = \"%s [adversarial loss: %f, acc: %f]\" % (log, loss, acc)\n",
    "        print(log)\n",
    "        if (i + 1) % save_interval == 0:\n",
    "            if (i + 1) == train_steps:\n",
    "                show = True\n",
    "            else:\n",
    "                show = False\n",
    "\n",
    "            # plot generator images on a periodic basis\n",
    "            plot_images(generator,\n",
    "                        noise_input=noise_input,\n",
    "                        show=show,\n",
    "                        step=(i + 1),\n",
    "                        model_name=model_name)\n",
    "   \n",
    "    # save the model after training the generator\n",
    "    # the trained generator can be reloaded for future MNIST digit generation\n",
    "    generator.save(model_name + \".h5\")\n",
    "\n",
    "\n",
    "def mi_loss(c, q_of_c_given_x):\n",
    "    \"\"\" Mutual information, Equation 5 in [2] , assuming H(c) is constant\"\"\"\n",
    "    # mi_loss = -c * log(Q(c|x))\n",
    "    return K.mean(-K.sum(K.log(q_of_c_given_x + K.epsilon()) * c, axis=1))\n",
    "\n",
    "\n",
    "def test_generator(generator, params, latent_size=100):\n",
    "    label, code1, code2, p1, p2 = params\n",
    "    noise_input = np.random.uniform(-1., 1., size=[16, latent_size])\n",
    "    step = 0\n",
    "    if label is None:\n",
    "        num_labels = 13\n",
    "        noise_label = np.eye(num_labels)[np.random.choice(num_labels, 16)]\n",
    "    else:\n",
    "        noise_label = np.zeros((16, 13))\n",
    "        noise_label[:,label] = 1\n",
    "        step = label\n",
    "\n",
    "    if code1 is None:\n",
    "        noise_code1 = np.ones((16, 1)) * -1\n",
    "    else:\n",
    "        if p1:\n",
    "            a = np.linspace(-1, 1, 16)\n",
    "            a = np.reshape(a, [16, 1])\n",
    "            noise_code1 = np.ones((16, 1)) * a\n",
    "        else:\n",
    "            noise_code1 = np.ones((16, 1)) * code1\n",
    "        print(noise_code1)\n",
    "\n",
    "    if code2 is None:\n",
    "        noise_code2 = np.ones((16, 1)) * -1\n",
    "    else:\n",
    "        if p2:\n",
    "            a = np.linspace(-1, 1, 16)\n",
    "            a = np.reshape(a, [16, 1])\n",
    "            noise_code2 = np.ones((16, 1)) * a\n",
    "        else:\n",
    "            noise_code2 = np.ones((16, 1)) * code2\n",
    "        print(noise_code2)\n",
    "\n",
    "    plot_images(generator,\n",
    "                    noise_input=noise_input,\n",
    "                    noise_label=noise_label,\n",
    "                    noise_codes=[noise_code1, noise_code2],\n",
    "                    show=False,\n",
    "                    step=step,\n",
    "                    model_name=\"test_outputs_{}\".format(label))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "sigmoid\n",
      "[<tf.Tensor 'activation_35/Identity:0' shape=(None, 1) dtype=float32>, <tf.Tensor 'label_7/Identity:0' shape=(None, 8) dtype=float32>, <tf.Tensor 'code0_14/Identity:0' shape=(None, 1) dtype=float32>, <tf.Tensor 'code1_14/Identity:0' shape=(None, 1) dtype=float32>, <tf.Tensor 'code2_14/Identity:0' shape=(None, 1) dtype=float32>, <tf.Tensor 'code3_14/Identity:0' shape=(None, 1) dtype=float32>, <tf.Tensor 'code4_14/Identity:0' shape=(None, 1) dtype=float32>, <tf.Tensor 'code5_14/Identity:0' shape=(None, 1) dtype=float32>, <tf.Tensor 'code6_14/Identity:0' shape=(None, 1) dtype=float32>, <tf.Tensor 'code7_14/Identity:0' shape=(None, 1) dtype=float32>, <tf.Tensor 'code8_14/Identity:0' shape=(None, 1) dtype=float32>, <tf.Tensor 'code9_14/Identity:0' shape=(None, 1) dtype=float32>] [<tf.Tensor 'discriminator_input_9:0' shape=(None, 256, 256, 3) dtype=float32>]\n",
      "Model: \"discriminator\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "discriminator_input (InputLayer [(None, 256, 256, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_56 (Conv2D)              (None, 128, 128, 32) 1568        discriminator_input[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_28 (LeakyReLU)      (None, 128, 128, 32) 0           conv2d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_28 (Dropout)            (None, 128, 128, 32) 0           leaky_re_lu_28[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_57 (Conv2D)              (None, 64, 64, 64)   32832       dropout_28[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_29 (LeakyReLU)      (None, 64, 64, 64)   0           conv2d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_29 (Dropout)            (None, 64, 64, 64)   0           leaky_re_lu_29[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_58 (Conv2D)              (None, 32, 32, 128)  131200      dropout_29[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_30 (LeakyReLU)      (None, 32, 32, 128)  0           conv2d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_30 (Dropout)            (None, 32, 32, 128)  0           leaky_re_lu_30[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_59 (Conv2D)              (None, 32, 32, 256)  524544      dropout_30[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_31 (LeakyReLU)      (None, 32, 32, 256)  0           conv2d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_31 (Dropout)            (None, 32, 32, 256)  0           leaky_re_lu_31[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "flatten_7 (Flatten)             (None, 262144)       0           dropout_31[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_99 (Dense)                (None, 128)          33554560    flatten_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_98 (Dense)                (None, 1)            262145      flatten_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_100 (Dense)               (None, 8)            1032        dense_99[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_101 (Dense)               (None, 1)            129         dense_99[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_102 (Dense)               (None, 1)            129         dense_99[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_103 (Dense)               (None, 1)            129         dense_99[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_104 (Dense)               (None, 1)            129         dense_99[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_105 (Dense)               (None, 1)            129         dense_99[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_106 (Dense)               (None, 1)            129         dense_99[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_107 (Dense)               (None, 1)            129         dense_99[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_108 (Dense)               (None, 1)            129         dense_99[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_109 (Dense)               (None, 1)            129         dense_99[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_110 (Dense)               (None, 1)            129         dense_99[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 1)            0           dense_98[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "label (Activation)              (None, 8)            0           dense_100[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "code0 (Activation)              (None, 1)            0           dense_101[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "code1 (Activation)              (None, 1)            0           dense_102[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "code2 (Activation)              (None, 1)            0           dense_103[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "code3 (Activation)              (None, 1)            0           dense_104[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "code4 (Activation)              (None, 1)            0           dense_105[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "code5 (Activation)              (None, 1)            0           dense_106[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "code6 (Activation)              (None, 1)            0           dense_107[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "code7 (Activation)              (None, 1)            0           dense_108[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "code8 (Activation)              (None, 1)            0           dense_109[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "code9 (Activation)              (None, 1)            0           dense_110[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 34,509,171\n",
      "Trainable params: 34,509,171\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"generator\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "z_input (InputLayer)            [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "labels (InputLayer)             [(None, 8)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "code0 (InputLayer)              [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "code1 (InputLayer)              [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "code2 (InputLayer)              [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "code3 (InputLayer)              [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "code4 (InputLayer)              [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "code5 (InputLayer)              [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "code6 (InputLayer)              [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "code7 (InputLayer)              [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "code8 (InputLayer)              [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "code9 (InputLayer)              [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 118)          0           z_input[0][0]                    \n",
      "                                                                 labels[0][0]                     \n",
      "                                                                 code0[0][0]                      \n",
      "                                                                 code1[0][0]                      \n",
      "                                                                 code2[0][0]                      \n",
      "                                                                 code3[0][0]                      \n",
      "                                                                 code4[0][0]                      \n",
      "                                                                 code5[0][0]                      \n",
      "                                                                 code6[0][0]                      \n",
      "                                                                 code7[0][0]                      \n",
      "                                                                 code8[0][0]                      \n",
      "                                                                 code9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_111 (Dense)               (None, 524288)       62390272    concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 524288)       2097152     dense_111[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_7 (Reshape)             (None, 64, 64, 128)  0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_28 (UpSampling2D) (None, 128, 128, 128 0           reshape_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_60 (Conv2D)              (None, 64, 64, 128)  262272      up_sampling2d_28[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 64, 64, 128)  512         conv2d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 64, 64, 128)  0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_29 (UpSampling2D) (None, 128, 128, 128 0           activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_61 (Conv2D)              (None, 64, 64, 64)   131136      up_sampling2d_29[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 64, 64, 64)   256         conv2d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 64, 64, 64)   0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_30 (UpSampling2D) (None, 128, 128, 64) 0           activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62 (Conv2D)              (None, 128, 128, 32) 32800       up_sampling2d_30[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 128, 128, 32) 128         conv2d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 128, 128, 32) 0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_31 (UpSampling2D) (None, 256, 256, 32) 0           activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_63 (Conv2D)              (None, 256, 256, 3)  1539        up_sampling2d_31[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 256, 256, 3)  12          conv2d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 256, 256, 3)  0           batch_normalization_39[0][0]     \n",
      "==================================================================================================\n",
      "Total params: 64,916,079\n",
      "Trainable params: 63,867,049\n",
      "Non-trainable params: 1,049,030\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"infogan_butterflies_darknet_generator_weirdweights\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "z_input (InputLayer)            [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "labels (InputLayer)             [(None, 8)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "code0 (InputLayer)              [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "code1 (InputLayer)              [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "code2 (InputLayer)              [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "code3 (InputLayer)              [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "code4 (InputLayer)              [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "code5 (InputLayer)              [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "code6 (InputLayer)              [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "code7 (InputLayer)              [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "code8 (InputLayer)              [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "code9 (InputLayer)              [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "generator (Model)               (None, 256, 256, 3)  64916079    z_input[0][0]                    \n",
      "                                                                 labels[0][0]                     \n",
      "                                                                 code0[0][0]                      \n",
      "                                                                 code1[0][0]                      \n",
      "                                                                 code2[0][0]                      \n",
      "                                                                 code3[0][0]                      \n",
      "                                                                 code4[0][0]                      \n",
      "                                                                 code5[0][0]                      \n",
      "                                                                 code6[0][0]                      \n",
      "                                                                 code7[0][0]                      \n",
      "                                                                 code8[0][0]                      \n",
      "                                                                 code9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "discriminator (Model)           [(None, 1), (None, 8 34509171    generator[1][0]                  \n",
      "==================================================================================================\n",
      "Total params: 99,425,250\n",
      "Trainable params: 63,867,049\n",
      "Non-trainable params: 35,558,201\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "train() takes 3 positional arguments but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-a8c4c0e36934>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_codes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: train() takes 3 positional arguments but 4 were given"
     ]
    }
   ],
   "source": [
    "model_name = \"infogan_butterflies_darknet_generator_weirdweights\"\n",
    "\n",
    "# load dataset\n",
    "(x_train, y_train), (_, _) = butterflies.load_data(path='../data/raw/butterflies.npz')\n",
    "\n",
    "# reshape data for CNN as (28, 28, 1) and normalize\n",
    "image_size = x_train.shape[1]\n",
    "x_train = np.reshape(x_train, [-1, image_size, image_size, 3])\n",
    "x_train = x_train.astype('float32')\n",
    "\n",
    "\n",
    "####\n",
    "#plotting sanity check\n",
    "\n",
    "os.makedirs(model_name, exist_ok=True)\n",
    "filename = os.path.join(model_name, \"example_real_inputs.png\")\n",
    "rows = int(math.sqrt(16))\n",
    "images = x_train[0:16]\n",
    "plt.figure(figsize=(10, 10))\n",
    "num_images = 16\n",
    "image_size = 256\n",
    "for i in range(num_images):\n",
    "    plt.subplot(rows, rows, i + 1)\n",
    "    image = np.reshape(images[i], [image_size, image_size, 3])\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "plt.savefig(filename)\n",
    "plt.close('all')\n",
    "####\n",
    "\n",
    "# train labels\n",
    "num_labels = len(np.unique(y_train))\n",
    "num_codes = 10\n",
    "print(num_labels)\n",
    "_, y_train = np.unique(y_train,return_inverse=True)\n",
    "y_train = to_categorical(y_train)\n",
    "\n",
    "# network parameters\n",
    "batch_size = 32\n",
    "train_steps = 100000\n",
    "lr = 2e-4\n",
    "decay = 6e-8\n",
    "input_shape = (image_size, image_size, 3)\n",
    "label_shape = (num_labels, )\n",
    "code_shape = (1, )\n",
    "latent_size = 100\n",
    "\n",
    "# build discriminator model\n",
    "inputs = Input(shape=input_shape, name='discriminator_input')\n",
    "# call discriminator builder with 4 outputs: source, label, and 2 codes\n",
    "discriminator = discriminator(inputs,\n",
    "                                  num_labels=num_labels,\n",
    "                                  num_codes=num_codes)\n",
    "print(discriminator.outputs,discriminator.inputs)\n",
    "# [1] uses Adam, but discriminator converges easily with RMSprop\n",
    "optimizer = RMSprop(lr=lr, decay=decay)\n",
    "# loss functions: 1) probability image is real (binary crossentropy)\n",
    "# 2) categorical cross entropy image label,\n",
    "# 3) and 4) mutual information loss\n",
    "loss = ['binary_crossentropy', 'categorical_crossentropy']\n",
    "for i in range(num_codes):\n",
    "    loss.append(mi_loss)\n",
    "# lamda or mi_loss weight is 0.5\n",
    "loss_weights = [1.0, 1.0] + [1.0 for i in range(num_codes)]\n",
    "discriminator.compile(loss=loss,\n",
    "                      loss_weights=loss_weights,\n",
    "                      optimizer=optimizer,\n",
    "                      metrics=['accuracy'])\n",
    "discriminator.summary()\n",
    "\n",
    "# build generator model\n",
    "input_shape = (latent_size, )\n",
    "inputs = Input(shape=input_shape, name='z_input')\n",
    "labels = Input(shape=label_shape, name='labels')\n",
    "codes = [Input(shape=code_shape, name=\"code{}\".format(i)) for i in range(num_codes)]\n",
    "# code1 = Input(shape=code_shape, name=\"code1\")\n",
    "# code2 = Input(shape=code_shape, name=\"code2\")\n",
    "# call generator with inputs, \n",
    "# labels and codes as total inputs to generator\n",
    "generator = generator(inputs,\n",
    "                          image_size,\n",
    "                          labels=labels,\n",
    "                          codes=codes)\n",
    "generator.summary()\n",
    "\n",
    "# build adversarial model = generator + discriminator\n",
    "optimizer = RMSprop(lr=lr*0.5, decay=decay*0.5)\n",
    "discriminator.trainable = False\n",
    "# total inputs = noise code, labels, and codes\n",
    "inputs = [inputs, labels] + codes\n",
    "adversarial = Model(inputs,\n",
    "                    discriminator(generator(inputs)),\n",
    "                    name=model_name)\n",
    "# same loss as discriminator\n",
    "adversarial.compile(loss=loss,\n",
    "                    loss_weights=loss_weights,\n",
    "                    optimizer=optimizer,\n",
    "                    metrics=['accuracy'])\n",
    "adversarial.summary()\n",
    "\n",
    "# train discriminator and adversarial networks\n",
    "models = (generator, discriminator, adversarial)\n",
    "data = (x_train, y_train)\n",
    "params = (batch_size, latent_size, train_steps, num_labels, model_name)\n",
    "train(models, data, params, num_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "generator = load_model(\"infogan_butterflies_darknet_generator.h5\")\n",
    "label = 0\n",
    "code1 = None\n",
    "code2 = True\n",
    "p1 = None\n",
    "p2 = True\n",
    "params = (label, code1, code2, p1, p2)\n",
    "test_generator(generator, params, latent_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
