{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import gc\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow.keras import Model, Sequential\n",
    "from functools import partial\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.callbacks import TensorBoard, LearningRateScheduler, EarlyStopping\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, Flatten, Conv2DTranspose,\\\n",
    "                                    Reshape, ReLU, LeakyReLU,BatchNormalization, Lambda, \\\n",
    "                                    UpSampling2D, Softmax, Input, Dropout, AvgPool2D, Concatenate, \\\n",
    "                                    LocallyConnected2D, SpatialDropout2D, Activation, InputLayer, \\\n",
    "                                    GaussianNoise\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import tensorflow.keras.backend as K\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "with np.load('/home/etheredge/Workspace/tcvae/data/gan_generated_guppies_fewer_samples.npz') as f:\n",
    "    gan_images, gan_labels = f['x_train'], f['y_train'].astype('int')\n",
    "real_images = np.load(\"/home/etheredge/Workspace/tcvae/data/imgs_trainVAE.npy\")\n",
    "real_labels = np.load(\"/home/etheredge/Workspace/tcvae/data/lines_trainVAE.npy\")\n",
    "brooks_images = np.load('/mnt/jordanlab/People/ietheredge/Guppies_Final/pattern_space/data/brooks_imgs_256.npy')\n",
    "brooks_attrs = np.load('/mnt/jordanlab/People/ietheredge/Guppies_Final/pattern_space/data/brooks_attrs.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images = gan_images\n",
    "# images_class = gan_labels\n",
    "images = np.concatenate([gan_images, real_images, brooks_images])\n",
    "# images = real_images\n",
    "# image_class = real_labels\n",
    "# images = brooks_images\n",
    "# images_class = brooks_attrs[:,1]\n",
    "images = images.astype('float32')\n",
    "images = images / images.max().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in np.random.randint(0,  len(images)-1, 5):\n",
    "    plt.imshow(images[i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaltAndPepper(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, ratio=0.9, **kwargs):\n",
    "        super(SaltAndPepper, self).__init__(**kwargs)\n",
    "        self.supports_masking = True\n",
    "        self.ratio = ratio\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        def noised():\n",
    "            shp = K.shape(inputs)[1:]\n",
    "            mask_select = K.random_binomial(shape=shp, p=self.ratio)\n",
    "            mask_noise = K.random_binomial(shape=shp, p=0.1) # salt and pepper have the same chance\n",
    "            out = (inputs * (mask_select)) + mask_noise\n",
    "            return out\n",
    "\n",
    "        return K.in_train_phase(noised(), inputs, training=training)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'ratio': self.ratio,\n",
    "                  'supports_masking': self.supports_masking}\n",
    "        base_config = super(SaltAndPepper, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalVariational(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, size, mu_prior=0., sigma_prior=1., add_kl=False, coef_kl = 1.0, add_mmd=True, use_homm=False, homm_order=3, lambda_mmd=1.0, kernel_f=None, name=None):\n",
    "        super().__init__(name=name)\n",
    "        self.add_kl = add_kl\n",
    "        self.add_mmd = add_mmd\n",
    "        self.use_homm = use_homm\n",
    "        self.order_homm = homm_order\n",
    "        self.lambda_mmd = lambda_mmd\n",
    "        if kernel_f is None:\n",
    "            self.kernel_f = self._rbf\n",
    "        else:\n",
    "            self.kernel_f = kernel_f\n",
    "\n",
    "        self.mu_layer = tf.keras.layers.Dense(size)\n",
    "        self.sigma_layer = tf.keras.layers.Dense(size)\n",
    "        self.mu_prior = tf.constant(mu_prior, dtype=tf.float32, shape=(size,))\n",
    "        self.sigma_prior = tf.constant(sigma_prior, dtype=tf.float32, shape=(size,))\n",
    "        self.coef_kl = tf.Variable(coef_kl, trainable=False, name='coef_kl')\n",
    "\n",
    "            \n",
    "    def _rbf(self, x, y):\n",
    "        x_size = tf.shape(x)[0]\n",
    "        y_size = tf.shape(y)[0]\n",
    "        dim = tf.shape(x)[1]\n",
    "        tiled_x = tf.tile(tf.reshape(x, tf.stack([x_size, 1, dim])), tf.stack([1, y_size, 1]))\n",
    "        tiled_y = tf.tile(tf.reshape(y, tf.stack([1, y_size, dim])), tf.stack([x_size, 1, 1]))\n",
    "        return tf.exp(-tf.reduce_mean(tf.square(tiled_x - tiled_y), axis=2) / tf.cast(dim, tf.float32))\n",
    "    \n",
    "    def _linear(self, x,y):\n",
    "        return tf.reduce_sum(tf.multiply(x,y))\n",
    "\n",
    "    def add_kl_divergence(self, q_mu, q_sigma, p_mu, p_sigma):\n",
    "        r = q_mu - p_mu\n",
    "        kl = tf.reduce_mean(self.coef_kl * tf.reduce_sum(tf.math.log(p_sigma) - tf.math.log(q_sigma) - .5 * (1. - (q_sigma**2 + r**2) / p_sigma**2), axis=1))\n",
    "        self.add_loss(kl)\n",
    "        self.add_metric(kl, 'mean', 'kl_divergence')\n",
    "\n",
    "    def add_mmd_discrepancy(self, z, z_prior):\n",
    "        k_prior = self.kernel_f(z_prior, z_prior)\n",
    "        k_post = self.kernel_f(z, z)\n",
    "        k_prior_post = self.kernel_f(z_prior, z)\n",
    "        mmd = tf.reduce_mean(k_prior) + tf.reduce_mean(k_post) - 2 * tf.reduce_mean(k_prior_post)\n",
    "        mmd = tf.multiply(self.lambda_mmd,  mmd, name='mmd')\n",
    "        self.add_loss(mmd)\n",
    "        self.add_metric(mmd, 'mean', 'mmd_discrepancy')\n",
    "\n",
    "    def add_homm_discrepancy(self, x, y, order=3, num=300000):\n",
    "        x = x - tf.reduce_mean(x, axis=0)\n",
    "        y = y - tf.reduce_mean(y, axis=0)\n",
    "        dim = tf.cast(x.shape[1], tf.int32)\n",
    "        index = tf.random.uniform(shape=(num, dim), minval=0, maxval=dim-1, dtype=tf.int32)\n",
    "        index = index[:, :order]\n",
    "        x = tf.transpose(x)\n",
    "        x = tf.gather(x, index)\n",
    "        y = tf.transpose(y)\n",
    "        y = tf.gather(y, index)\n",
    "        ho_x = tf.reduce_prod(x, axis=1)\n",
    "        ho_x = tf.reduce_mean(ho_x, axis=1)\n",
    "        ho_y = tf.reduce_prod(y, axis=1)\n",
    "        ho_y = tf.reduce_mean(ho_y, axis=1)\n",
    "        homm_loss = tf.reduce_mean(tf.square(tf.subtract(ho_x, ho_y)))\n",
    "        homm_loss = tf.multiply(self.lambda_mmd,  homm_loss, name='homm')\n",
    "        self.add_loss(homm_loss)\n",
    "        self.add_metric(homm_loss, 'mean', 'homm_discrepancy')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        mu = self.mu_layer(inputs)\n",
    "        log_sigma =  self.sigma_layer(inputs)\n",
    "        sigma_square = tf.exp(log_sigma)\n",
    "        if self.add_kl:\n",
    "            self.add_kl_divergence(mu, sigma_square, self.mu_prior, self.sigma_prior)\n",
    "            \n",
    "        z = mu + sigma_square * tf.random.normal(tf.shape(sigma_square))\n",
    "        if self.add_mmd:\n",
    "            if self.use_homm:\n",
    "                z_prior = tfp.distributions.MultivariateNormalDiag(self.mu_prior, self.sigma_prior).sample(tf.shape(z)[0])\n",
    "                z_adapt = tf.tanh(z)\n",
    "                z_prior_adapt = tf.tanh(z_prior)\n",
    "                self.add_homm_discrepancy(z_adapt, z_prior_adapt, order=self.order_homm)\n",
    "\n",
    "            else:\n",
    "                z_prior = tfp.distributions.MultivariateNormalDiag(self.mu_prior, self.sigma_prior).sample(tf.shape(z)[0])\n",
    "                self.add_mmd_discrepancy(z, z_prior)\n",
    "\n",
    "        return z\n",
    "\n",
    "    def get_config(self):\n",
    "        base_config = super(NormalVariational, self).get_config()\n",
    "        config = {\n",
    "            'add_kl': self.add_kl,\n",
    "            'add_mmd': self.add_mmd,\n",
    "            'use_homm': self.use_homm,\n",
    "            'order_homm': self.order_homm,\n",
    "            'lambda_mmd': self.lambda_mmd,\n",
    "        }\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_encoder(latent_size):\n",
    "    \n",
    "    inputs = Input((256,256,3))\n",
    "    with tf.name_scope('noise'):\n",
    "        noise_layers = Sequential([\n",
    "            SaltAndPepper(),\n",
    "            GaussianNoise(0.1)\n",
    "            ], name='noise')\n",
    "        noisy_inputs = noise_layers(inputs)\n",
    "\n",
    "    with tf.name_scope('h_1'):\n",
    "        h_1_layers = Sequential([ \n",
    "            Input((256, 256, 3)),\n",
    "            Conv2D(64, 4, 2, kernel_regularizer=l2(2.5e-5),\n",
    "                   kernel_initializer=tf.random_normal_initializer(stddev=0.02)),\n",
    "            BatchNormalization(),\n",
    "            LeakyReLU(),\n",
    "            Conv2D(64, 4, 1, kernel_regularizer=l2(2.5e-5),\n",
    "                   kernel_initializer=tf.random_normal_initializer(stddev=0.02)),\n",
    "            BatchNormalization(),\n",
    "            LeakyReLU()], name='h_1')\n",
    "        h_1 = h_1_layers(noisy_inputs)\n",
    "        h_1_flatten = Flatten()(h_1)\n",
    "\n",
    "    with tf.name_scope('h_2'):\n",
    "        h_2_layers = Sequential([ \n",
    "            Conv2D(128, 4, 2, kernel_regularizer=l2(2.5e-5),\n",
    "                   kernel_initializer=tf.random_normal_initializer(stddev=0.02)),\n",
    "            BatchNormalization(),\n",
    "            LeakyReLU(),\n",
    "            Conv2D(128, 4, 1, kernel_regularizer=l2(2.5e-5),\n",
    "                   kernel_initializer=tf.random_normal_initializer(stddev=0.02)),\n",
    "            BatchNormalization(),\n",
    "            LeakyReLU(),\n",
    "            Conv2D(256, 4, 2, kernel_regularizer=l2(2.5e-5),\n",
    "                   kernel_initializer=tf.random_normal_initializer(stddev=0.02)),\n",
    "            BatchNormalization(),\n",
    "            LeakyReLU()], name='h_2')\n",
    "        h_2 = h_2_layers(h_1)\n",
    "        h_2_flatten = Flatten()(h_2)\n",
    "\n",
    "    with tf.name_scope('h_3'):\n",
    "        h_3_layers = Sequential([ \n",
    "            Conv2D(256, 4, 1, kernel_regularizer=l2(2.5e-5),\n",
    "                   kernel_initializer=tf.random_normal_initializer(stddev=0.02)),\n",
    "            BatchNormalization(),\n",
    "            LeakyReLU(),\n",
    "            Conv2D(512, 4, 2, kernel_regularizer=l2(2.5e-5),\n",
    "                   kernel_initializer=tf.random_normal_initializer(stddev=0.02)),\n",
    "            BatchNormalization(),\n",
    "            LeakyReLU(),\n",
    "            Conv2D(512, 4, 1, kernel_regularizer=l2(2.5e-5),\n",
    "                   kernel_initializer=tf.random_normal_initializer(stddev=0.02)),\n",
    "            BatchNormalization(),\n",
    "            LeakyReLU()], name='h_3')\n",
    "        h_3 = h_3_layers(h_2)\n",
    "        h_3_flatten = Flatten()(h_3)\n",
    "\n",
    "    with tf.name_scope('h_4'):\n",
    "        h_4_layers = Sequential([ \n",
    "            Dense(1024, kernel_regularizer=l2(2.5e-5),\n",
    "                   kernel_initializer=tf.random_normal_initializer(stddev=0.02)),\n",
    "            BatchNormalization(),\n",
    "            LeakyReLU(),\n",
    "            Dense(1024, kernel_regularizer=l2(2.5e-5),\n",
    "                   kernel_initializer=tf.random_normal_initializer(stddev=0.02)),\n",
    "            BatchNormalization(),\n",
    "            LeakyReLU()], name='h_4')\n",
    "        h_4 = h_4_layers(h_3)\n",
    "        h_4_flatten = Flatten()(h_4)\n",
    "\n",
    "    return Model(inputs, [h_1_flatten, h_2_flatten, h_3_flatten, h_4_flatten], name='encoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_decoder(latent_dim1, latent_dim2, latent_dim3, latent_dim4):\n",
    "    z_1_input, z_2_input, z_3_input, z_4_input = Input((latent_dim1,), name='z_1'), Input((latent_dim2,), name='z_2'), Input((latent_dim3,), name='z_3'), Input((latent_dim4,), name='z_4')\n",
    "    \n",
    "    with tf.name_scope('z_tilde_4'):\n",
    "        z_4 = z_4_input\n",
    "        z_tilde_4_layers = Sequential([\n",
    "            Dense(1024, kernel_regularizer=l2(2.5e-5)),\n",
    "            BatchNormalization(),\n",
    "            ReLU(),\n",
    "            Dense(1024, kernel_regularizer=l2(2.5e-5)),\n",
    "            BatchNormalization(),\n",
    "            ReLU(),\n",
    "            Dense(16*16*512, kernel_regularizer=l2(2.5e-5)),\n",
    "            BatchNormalization(),\n",
    "            ReLU(),\n",
    "            Reshape((16,16,512))], name='z_tilde_4')\n",
    "        z_tilde_4 = z_tilde_4_layers(z_4)\n",
    "\n",
    "    with tf.name_scope('z_tilde_3'):\n",
    "        z_3 = Dense(16*16*512, kernel_regularizer=l2(2.5e-5))(z_3_input)\n",
    "        z_3 = BatchNormalization()(z_3)\n",
    "        z_3 = ReLU()(z_3)\n",
    "        z_3 = Reshape((16,16,512))(z_3)\n",
    "        z_tilde_3_layers = Sequential([\n",
    "            Conv2DTranspose(512, 4, 1, padding='same', kernel_regularizer=l2(2.5e-5)),\n",
    "            BatchNormalization(),\n",
    "            ReLU(),\n",
    "            Conv2DTranspose(256, 4, 2, padding='same', kernel_regularizer=l2(2.5e-5)),\n",
    "            BatchNormalization(),\n",
    "            ReLU(),\n",
    "            Conv2DTranspose(256, 4, 1, padding='same', kernel_regularizer=l2(2.5e-5)),\n",
    "            BatchNormalization(),\n",
    "            ReLU()], name='z_tilde_3')\n",
    "        input_z_tilde_3 = Concatenate()([z_tilde_4, z_3])\n",
    "        z_tilde_3 = z_tilde_3_layers(input_z_tilde_3)\n",
    "        \n",
    "    with tf.name_scope('z_tilde_2'):\n",
    "        z_2 = Dense(32*32*256, kernel_regularizer=l2(2.5e-5))(z_2_input)\n",
    "        z_2 = BatchNormalization()(z_2)\n",
    "        z_2 = ReLU()(z_2)\n",
    "        z_2 = Reshape((32,32,256))(z_2)\n",
    "        z_tilde_2_layers = Sequential([\n",
    "            Conv2DTranspose(128, 4, 2, padding='same', kernel_regularizer=l2(2.5e-5)),\n",
    "            BatchNormalization(),\n",
    "            ReLU(),\n",
    "            Conv2DTranspose(128, 4, 1, padding='same', kernel_regularizer=l2(2.5e-5)),\n",
    "            BatchNormalization(),\n",
    "            ReLU(),\n",
    "            Conv2DTranspose(64, 4, 2, padding='same', kernel_regularizer=l2(2.5e-5)),\n",
    "            BatchNormalization(),\n",
    "            ReLU()], name='z_tilde_2')\n",
    "        input_z_tilde_2 = Concatenate()([z_tilde_3, z_2])\n",
    "        z_tilde_2 =  z_tilde_2_layers(input_z_tilde_2)\n",
    "    \n",
    "    with tf.name_scope('z_tilde_1'):\n",
    "        z_1 = Dense(128*128*64, kernel_regularizer=l2(2.5e-5))(z_1_input)\n",
    "        z_1 = BatchNormalization()(z_1)\n",
    "        z_1 = ReLU()(z_1)\n",
    "        z_1 = Reshape((128,128,64))(z_1)\n",
    "        z_tilde_1_layers = Sequential([\n",
    "            Conv2DTranspose(128, 4, 2, padding='same', kernel_regularizer=l2(2.5e-5)),\n",
    "            Conv2DTranspose(3, 4, 1, padding='same', kernel_regularizer=l2(2.5e-5)),\n",
    "            Activation('sigmoid')], name='z_tilde_1')\n",
    "        input_z_tilde_1 = Concatenate()([z_tilde_2, z_1])\n",
    "        decoder =  z_tilde_1_layers(input_z_tilde_1)\n",
    "\n",
    "    return Model([z_1_input, z_2_input, z_3_input, z_4_input], decoder, name='decoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_vlae(latent_size, use_perceptual_loss=False):\n",
    "    with tf.name_scope('encoder'):\n",
    "        encoder = make_encoder(latent_size)\n",
    "    with tf.name_scope('decoder'):\n",
    "        decoder = make_decoder(latent_size, latent_size, latent_size, latent_size)\n",
    "    inputs = Input((256,256,3))\n",
    "    h_1, h_2, h_3, h_4 = encoder(inputs)\n",
    "    z_1 = NormalVariational(latent_size, add_kl=False, coef_kl=1., add_mmd=True, lambda_mmd=5000., name='z_1_latent')(h_1)\n",
    "    z_2 = NormalVariational(latent_size, add_kl=False, coef_kl=1., add_mmd=True, lambda_mmd=5000., name='z_2_latent')(h_2)\n",
    "    z_3 = NormalVariational(latent_size, add_kl=False, coef_kl=1., add_mmd=True, lambda_mmd=5000., name='z_3_latent')(h_3)\n",
    "    z_4 = NormalVariational(latent_size, add_kl=False, coef_kl=1., add_mmd=True, lambda_mmd=5000., name='z_4_latent')(h_4)\n",
    "    decoded = decoder([z_1, z_2, z_3, z_4])\n",
    "    vlae = Model(inputs, decoded, name='vlae')\n",
    "    return vlae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KLWarmUp(tf.keras.callbacks.Callback):\n",
    "    \n",
    "    def __init__(self, vlae, n_iter, start=0.0, stop=1.0,  n_cycle=4, ratio=0.5):\n",
    "        self.frange = frange_cycle_linear(n_iter, start=start, stop=stop,  n_cycle=n_cycle, ratio=ratio)\n",
    "        self.vlae = vlae\n",
    "        self.epoch = 0\n",
    "        \n",
    "        \n",
    "    def on_epoch_end(self, *args, **kwargs):\n",
    "        new_coef = self.frange[self.epoch]\n",
    "        self.epoch += 1\n",
    "        coefs = [self.vlae.get_layer(f'z_{i+1}_latent').coef_kl for i in range(3)]\n",
    "        for coef in coefs:\n",
    "            coef.assign(new_coef)\n",
    "    \n",
    "    @staticmethod\n",
    "    def frange_cycle_linear(n_iter, start=0.0, stop=1.0,  n_cycle=4, ratio=0.5):\n",
    "        L = np.ones(n_iter) * stop\n",
    "        period = n_iter/n_cycle\n",
    "        step = (stop-start)/(period*ratio) # linear schedule\n",
    "\n",
    "        for c in range(n_cycle):\n",
    "            v, i = start, 0\n",
    "            while v <= stop and (int(i+c*period) < n_iter):\n",
    "                L[int(i+c*period)] = v\n",
    "                v += step\n",
    "                i += 1\n",
    "        return L "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %load_ext tensorboard\n",
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_size = 10\n",
    "vlae = make_vlae(latent_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 500\n",
    "tb = TensorBoard(write_images=False, write_graph=True, histogram_freq=25)\n",
    "es = EarlyStopping(min_delta=0.0001, patience=15)\n",
    "# klwarmup = KLWarmUp(vlae, epochs)\n",
    "lr_epochs = 10 ** np.linspace(-5, -8, epochs)\n",
    "lrsched = LearningRateScheduler(lambda i: lr_epochs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "vlae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(x,xhat):\n",
    "    return  .5 * tf.losses.mean_squared_error(Flatten()(x), Flatten()(xhat)) * np.prod(images[0].shape)\n",
    "vlae.compile(tf.keras.optimizers.Adam(), loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vlae.fit(images, images, validation_split=0.05, batch_size=20, epochs=epochs, callbacks=[tb, lrsched])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "tf.saved_model.save(vlae, 'models/vlae_mmd_all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample = [np.random.multivariate_normal([0] * latent_size, np.diag([1] * latent_size), 20)] * 4\n",
    "generated = vlae.get_layer('decoder').predict(sample)\n",
    "generated = generated.reshape((20, 256, 256,3))\n",
    "\n",
    "for i in range(20):\n",
    "    img = generated[i]\n",
    "    plt.imshow(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_images = np.load(\"/home/etheredge/Workspace/tcvae/data/imgs_trainVAE.npy\")\n",
    "real_labels = np.load(\"/home/etheredge/Workspace/tcvae/data/lines_trainVAE.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# real_images = brooks_images\n",
    "# real_labels = brooks_attrs[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "import matplotlib as mpl\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_0 = []\n",
    "h_1 = []\n",
    "h_2 = []\n",
    "h_3 = []\n",
    "for image in gan_images:\n",
    "    x = vlae.get_layer('encoder').predict(np.expand_dims(image, axis=0));\n",
    "    h_0.append(vlae.get_layer('z_1_latent')(x[0]))\n",
    "    h_1.append(vlae.get_layer('z_2_latent')(x[1]))\n",
    "    h_2.append(vlae.get_layer('z_3_latent')(x[2]))\n",
    "    h_3.append(vlae.get_layer('z_4_latent')(x[3]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "h_3 = np.array(h_3)\n",
    "h_3 = h_3.reshape(gan_images.shape[0],latent_size)\n",
    "\n",
    "h_2 = np.array(h_2)\n",
    "h_2 = h_2.reshape(gan_images.shape[0],latent_size)\n",
    "\n",
    "h_1 = np.array(h_1)\n",
    "h_1 = h_1.reshape(gan_images.shape[0],latent_size)\n",
    "\n",
    "h_0 = np.array(h_0)\n",
    "h_0 = h_0.reshape(gan_images.shape[0],latent_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('data/interim/h_3_gan', h_3)\n",
    "np.save('data/interim/h_2_gan', h_2)\n",
    "np.save('data/interim/h_1_gan', h_1)\n",
    "np.save('data/interim/h_0_gan', h_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(100,25))\n",
    "for i, h in enumerate([h_0, h_1, h_2, h_3]):\n",
    "    ax = plt.subplot(1,4,i+1,sharex, sharey='all')\n",
    "    embedding = TSNE(n_components=2).fit_transform(Z)\n",
    "    scatter = imscatter(embedding[:, 0], embedding[:, 1], gan_images, zoom=0.1, ax=ax)\n",
    "    ax.set_title('Level {}'.format(i+1))\n",
    "fig = plt.gcf()\n",
    "fig.savefig('reports/figures/panels/vlae_embedding_gan_images.pdf', dpi=300, bbox_inches = 'tight',\n",
    "    pad_inches = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
    "plt.style.use('classic')\n",
    "plt.rcParams.update({'font.size': 30})\n",
    "\n",
    "Z = h_3\n",
    "# reducer = umap.UMAP(random_state=2, n_neighbors=30)\n",
    "# embedding = reducer.fit_transform(Z)\n",
    "embedding = TSNE(n_components=2).fit_transform(Z)\n",
    "def imscatter(x, y, image, ax=None, zoom=1):\n",
    "    if ax is None:\n",
    "        ax = plt.gca();\n",
    "    try:\n",
    "        image = plt.imread(image);\n",
    "    except TypeError:\n",
    "        # Likely already an array...\n",
    "        pass\n",
    "    x, y = np.atleast_1d(x, y);\n",
    "    artists = [];\n",
    "    for i, (x0, y0) in enumerate(zip(x, y)):\n",
    "        im = OffsetImage(image[i], zoom=zoom);\n",
    "        ab = AnnotationBbox(im, (x0, y0), xycoords='data', frameon=False);\n",
    "        artists.append(ax.add_artist(ab));\n",
    "    ax.update_datalim(np.column_stack([x, y]));\n",
    "    ax.autoscale();\n",
    "    ax.grid(False);\n",
    "    return artists\n",
    "fig, ax = plt.subplots(figsize=(100,25));\n",
    "imscatter(embedding[:, 0], embedding[:, 1], real_images, zoom=0.1, ax=ax);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polyfit2d(x, y, z, order=3):\n",
    "    ncols = (order + 1)**2\n",
    "    G = np.zeros((x.size, ncols))\n",
    "    ij = itertools.product(range(order+1), range(order+1))\n",
    "    for k, (i,j) in enumerate(ij):\n",
    "        G[:,k] = x**i * y**j\n",
    "    m, _, _, _ = np.linalg.lstsq(G, z)\n",
    "    return m\n",
    "\n",
    "def polyval2d(x, y, m):\n",
    "    order = int(np.sqrt(len(m))) - 1\n",
    "    ij = itertools.product(range(order+1), range(order+1))\n",
    "    z = np.zeros_like(x)\n",
    "    for a, (i,j) in zip(m, ij):\n",
    "        z += a * x**i * y**j\n",
    "    return z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = brooks_attrs[:,1]\n",
    "# Z = np.concatenate([h_0, h_1, h_2, h_3],axis=1)\n",
    "Z = h_3\n",
    "# Z = attrs[:,2:]\n",
    "# reducer = umap.UMAP(random_state=2)\n",
    "reducer = PCA(n_components=2)\n",
    "embedding = reducer.fit_transform(Z)\n",
    "x = embedding[:, 0]\n",
    "y = embedding[:, 1]\n",
    "z = L\n",
    "m = polyfit2d(x, y, z)\n",
    "nx, ny = 20, 20\n",
    "xx, yy = np.meshgrid(np.linspace(x.min(), x.max(), nx), \n",
    "                     np.linspace(y.min(), y.max(), ny))\n",
    "zz = polyval2d(xx, yy, m)\n",
    "\n",
    "plt.scatter(x, y, c=z)\n",
    "plt.imshow(zz, extent=(x.min(), y.max(), x.max(), y.min()))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future improvements\n",
    "* Use the label as additional information to improve the recostruction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
